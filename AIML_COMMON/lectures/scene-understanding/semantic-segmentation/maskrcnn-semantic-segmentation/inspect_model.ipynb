{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask R-CNN - Inspect Trained Model\n",
    "\n",
    "Code and visualizations to test, debug, and evaluate the Mask R-CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Path to Shapes trained weights\n",
    "SHAPES_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_shapes.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run one of the code blocks\n",
    "\n",
    "# Shapes toy dataset\n",
    "# import shapes\n",
    "# config = shapes.ShapesConfig()\n",
    "\n",
    "# MS COCO Dataset\n",
    "import coco\n",
    "config = coco.CocoConfig()\n",
    "COCO_DIR = \"path to COCO dataset\"  # TODO: enter value here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[256 256]\n",
      " [128 128]\n",
      " [ 64  64]\n",
      " [ 32  32]\n",
      " [ 16  16]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [ 0.1  0.1  0.2  0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.5\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.002\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [ 123.7  116.8  103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           coco\n",
      "NUM_CLASSES                    81\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              2\n",
      "RPN_BBOX_STD_DEV               [ 0.1  0.1  0.2  0.2]\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TRAIN_ROIS_PER_IMAGE           128\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Override the training configurations with a few\n",
    "# changes for inferencing.\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.86s)\n",
      "creating index...\n",
      "index created!\n",
      "Images: 35185\n",
      "Classes: ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "# Build validation dataset\n",
    "if config.NAME == 'shapes':\n",
    "    dataset = shapes.ShapesDataset()\n",
    "    dataset.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "elif config.NAME == \"coco\":\n",
    "    dataset = coco.CocoDataset()\n",
    "    dataset.load_coco(COCO_DIR, \"minival\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
    "                              config=config)\n",
    "\n",
    "# Set weights file path\n",
    "if config.NAME == \"shapes\":\n",
    "    weights_path = SHAPES_MODEL_PATH\n",
    "elif config.NAME == \"coco\":\n",
    "    weights_path = COCO_MODEL_PATH\n",
    "# Or, uncomment to load the last model you trained\n",
    "# weights_path = model.find_last()\n",
    "\n",
    "# Load weights\n",
    "print(\"Loading weights \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = random.choice(dataset.image_ids)\n",
    "image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "info = dataset.image_info[image_id]\n",
    "print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n",
    "                                       dataset.image_reference(image_id)))\n",
    "# Run object detection\n",
    "results = model.detect([image], verbose=1)\n",
    "\n",
    "# Display results\n",
    "ax = get_ax(1)\n",
    "r = results[0]\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset.class_names, r['scores'], ax=ax,\n",
    "                            title=\"Predictions\")\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f08c3caadd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw precision-recall curve\n",
    "AP, precisions, recalls, overlaps = utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                                          r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
    "visualize.plot_precision_recall(AP, precisions, recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid of ground truth objects and their predictions\n",
    "visualize.plot_overlaps(gt_class_id, r['class_ids'], r['scores'],\n",
    "                        overlaps, dataset.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute mAP @ IoU=50 on Batch of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/ndimage/interpolation.py:600: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP @ IoU=50:  0.656323084916\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-style Average Precision\n",
    "def compute_batch_ap(image_ids):\n",
    "    APs = []\n",
    "    for image_id in image_ids:\n",
    "        # Load image\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(dataset, config,\n",
    "                                   image_id, use_mini_mask=False)\n",
    "        # Run object detection\n",
    "        results = model.detect([image], verbose=0)\n",
    "        # Compute AP\n",
    "        r = results[0]\n",
    "        AP, precisions, recalls, overlaps =\\\n",
    "            utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                              r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
    "        APs.append(AP)\n",
    "    return APs\n",
    "\n",
    "# Pick a set of random images\n",
    "image_ids = np.random.choice(dataset.image_ids, 10)\n",
    "APs = compute_batch_ap(image_ids)\n",
    "print(\"mAP @ IoU=50: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Region Proposal Network\n",
    "\n",
    "The Region Proposal Network (RPN) runs a lightweight binary classifier on a lot of boxes (anchors) over the image and returns object/no-object scores. Anchors with high *objectness* score (positive anchors) are passed to the stage two to be classified.\n",
    "\n",
    "Often, even positive anchors don't cover objects fully. So the RPN also regresses a refinement (a delta in location and size) to be applied to the anchors to shift it and resize it a bit to the correct boundaries of the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a RPN Targets\n",
    "\n",
    "The RPN targets are the training values for the RPN. To generate the targets, we start with a grid of anchors that cover the full image at different scales, and then we compute the IoU of the anchors with ground truth object. Positive anchors are those that have an IoU >= 0.7 with any ground truth object, and negative anchors are those that don't cover any object by more than 0.3 IoU. Anchors in between (i.e. cover an object by IoU >= 0.3 but < 0.7) are considered neutral and excluded from training.\n",
    "\n",
    "To train the RPN regressor, we also compute the shift and resizing needed to make the anchor cover the ground truth object completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_rpn_match         shape: (65472,)              min:   -1.00000  max:    1.00000\n",
      "target_rpn_bbox          shape: (256, 4)              min:   -5.19860  max:    2.59641\n",
      "positive_anchors         shape: (14, 4)               min:    5.49033  max:  973.25483\n",
      "negative_anchors         shape: (242, 4)              min:  -22.62742  max: 1038.62742\n",
      "neutral anchors          shape: (65216, 4)            min: -362.03867  max: 1258.03867\n",
      "refined_anchors          shape: (14, 4)               min:    0.00000  max: 1023.99994\n"
     ]
    }
   ],
   "source": [
    "# Generate RPN trainig targets\n",
    "# target_rpn_match is 1 for positive anchors, -1 for negative anchors\n",
    "# and 0 for neutral anchors.\n",
    "target_rpn_match, target_rpn_bbox = modellib.build_rpn_targets(\n",
    "    image.shape, model.anchors, gt_class_id, gt_bbox, model.config)\n",
    "log(\"target_rpn_match\", target_rpn_match)\n",
    "log(\"target_rpn_bbox\", target_rpn_bbox)\n",
    "\n",
    "positive_anchor_ix = np.where(target_rpn_match[:] == 1)[0]\n",
    "negative_anchor_ix = np.where(target_rpn_match[:] == -1)[0]\n",
    "neutral_anchor_ix = np.where(target_rpn_match[:] == 0)[0]\n",
    "positive_anchors = model.anchors[positive_anchor_ix]\n",
    "negative_anchors = model.anchors[negative_anchor_ix]\n",
    "neutral_anchors = model.anchors[neutral_anchor_ix]\n",
    "log(\"positive_anchors\", positive_anchors)\n",
    "log(\"negative_anchors\", negative_anchors)\n",
    "log(\"neutral anchors\", neutral_anchors)\n",
    "\n",
    "# Apply refinement deltas to positive anchors\n",
    "refined_anchors = utils.apply_box_deltas(\n",
    "    positive_anchors,\n",
    "    target_rpn_bbox[:positive_anchors.shape[0]] * model.config.RPN_BBOX_STD_DEV)\n",
    "log(\"refined_anchors\", refined_anchors, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display positive anchors before refinement (dotted) and\n",
    "# after refinement (solid).\n",
    "visualize.draw_boxes(image, boxes=positive_anchors, refined_boxes=refined_anchors, ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b RPN Predictions\n",
    "\n",
    "Here we run the RPN graph and display its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpn_class                shape: (1, 65472, 2)         min:    0.00000  max:    1.00000\n",
      "pre_nms_anchors          shape: (1, 10000, 4)         min: -362.03867  max: 1258.03870\n",
      "refined_anchors          shape: (1, 10000, 4)         min: -1385.67920  max: 2212.44043\n",
      "refined_anchors_clipped  shape: (1, 10000, 4)         min:    0.00000  max: 1024.00000\n",
      "post_nms_anchor_ix       shape: (1000,)               min:    0.00000  max: 1477.00000\n",
      "proposals                shape: (1, 1000, 4)          min:    0.00000  max:    1.00000\n"
     ]
    }
   ],
   "source": [
    "# Run RPN sub-graph\n",
    "pillar = model.keras_model.get_layer(\"ROI\").output  # node to start searching from\n",
    "\n",
    "# TF 1.4 and 1.9 introduce new versions of NMS. Search for all names to support TF 1.3~1.10\n",
    "nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression:0\")\n",
    "if nms_node is None:\n",
    "    nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV2:0\")\n",
    "if nms_node is None: #TF 1.9-1.10\n",
    "    nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV3:0\")\n",
    "\n",
    "rpn = model.run_graph([image], [\n",
    "    (\"rpn_class\", model.keras_model.get_layer(\"rpn_class\").output),\n",
    "    (\"pre_nms_anchors\", model.ancestor(pillar, \"ROI/pre_nms_anchors:0\")),\n",
    "    (\"refined_anchors\", model.ancestor(pillar, \"ROI/refined_anchors:0\")),\n",
    "    (\"refined_anchors_clipped\", model.ancestor(pillar, \"ROI/refined_anchors_clipped:0\")),\n",
    "    (\"post_nms_anchor_ix\", nms_node),\n",
    "    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top anchors by score (before refinement)\n",
    "limit = 100\n",
    "sorted_anchor_ids = np.argsort(rpn['rpn_class'][:,:,1].flatten())[::-1]\n",
    "visualize.draw_boxes(image, boxes=model.anchors[sorted_anchor_ids[:limit]], ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show top anchors with refinement. Then with clipping to image boundaries\n",
    "limit = 50\n",
    "ax = get_ax(1, 2)\n",
    "pre_nms_anchors = utils.denorm_boxes(rpn[\"pre_nms_anchors\"][0], image.shape[:2])\n",
    "refined_anchors = utils.denorm_boxes(rpn[\"refined_anchors\"][0], image.shape[:2])\n",
    "refined_anchors_clipped = utils.denorm_boxes(rpn[\"refined_anchors_clipped\"][0], image.shape[:2])\n",
    "visualize.draw_boxes(image, boxes=pre_nms_anchors[:limit],\n",
    "                     refined_boxes=refined_anchors[:limit], ax=ax[0])\n",
    "visualize.draw_boxes(image, refined_boxes=refined_anchors_clipped[:limit], ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show refined anchors after non-max suppression\n",
    "limit = 50\n",
    "ixs = rpn[\"post_nms_anchor_ix\"][:limit]\n",
    "visualize.draw_boxes(image, refined_boxes=refined_anchors_clipped[ixs], ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final proposals\n",
    "# These are the same as the previous step (refined anchors \n",
    "# after NMS) but with coordinates normalized to [0, 1] range.\n",
    "limit = 50\n",
    "# Convert back to image coordinates for display\n",
    "h, w = config.IMAGE_SHAPE[:2]\n",
    "proposals = rpn['proposals'][0, :limit] * np.array([h, w, h, w])\n",
    "visualize.draw_boxes(image, refined_boxes=proposals, ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Anchors (65472)       Recall: 0.400  Positive anchors: 8\n",
      "Refined Anchors (10000)   Recall: 0.900  Positive anchors: 65\n",
      "Post NMS Anchors (   50)  Recall: 0.800  Positive anchors: 9\n"
     ]
    }
   ],
   "source": [
    "# Measure the RPN recall (percent of objects covered by anchors)\n",
    "# Here we measure recall for 3 different methods:\n",
    "# - All anchors\n",
    "# - All refined anchors\n",
    "# - Refined anchors after NMS\n",
    "iou_threshold = 0.7\n",
    "\n",
    "recall, positive_anchor_ids = utils.compute_recall(model.anchors, gt_bbox, iou_threshold)\n",
    "print(\"All Anchors ({:5})       Recall: {:.3f}  Positive anchors: {}\".format(\n",
    "    model.anchors.shape[0], recall, len(positive_anchor_ids)))\n",
    "\n",
    "recall, positive_anchor_ids = utils.compute_recall(rpn['refined_anchors'][0], gt_bbox, iou_threshold)\n",
    "print(\"Refined Anchors ({:5})   Recall: {:.3f}  Positive anchors: {}\".format(\n",
    "    rpn['refined_anchors'].shape[1], recall, len(positive_anchor_ids)))\n",
    "\n",
    "recall, positive_anchor_ids = utils.compute_recall(proposals, gt_bbox, iou_threshold)\n",
    "print(\"Post NMS Anchors ({:5})  Recall: {:.3f}  Positive anchors: {}\".format(\n",
    "    proposals.shape[0], recall, len(positive_anchor_ids)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Proposal Classification\n",
    "\n",
    "This stage takes the region proposals from the RPN and classifies them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Proposal Classification\n",
    "\n",
    "Run the classifier heads on proposals to generate class propbabilities and bounding box regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposals                shape: (1, 1000, 4)          min:    0.00000  max:    1.00000\n",
      "probs                    shape: (1, 1000, 81)         min:    0.00000  max:    0.99999\n",
      "deltas                   shape: (1, 1000, 81, 4)      min:   -3.26400  max:    3.83929\n",
      "masks                    shape: (1, 100, 28, 28, 81)  min:    0.00000  max:    0.99984\n",
      "detections               shape: (1, 100, 6)           min:    0.00000  max:  844.00000\n"
     ]
    }
   ],
   "source": [
    "# Get input and output to classifier and mask heads.\n",
    "mrcnn = model.run_graph([image], [\n",
    "    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n",
    "    (\"probs\", model.keras_model.get_layer(\"mrcnn_class\").output),\n",
    "    (\"deltas\", model.keras_model.get_layer(\"mrcnn_bbox\").output),\n",
    "    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n",
    "    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detection class IDs. Trim zero padding.\n",
    "det_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\n",
    "det_count = np.where(det_class_ids == 0)[0][0]\n",
    "det_class_ids = det_class_ids[:det_count]\n",
    "detections = mrcnn['detections'][0, :det_count]\n",
    "\n",
    "print(\"{} detections: {}\".format(\n",
    "    det_count, np.array(dataset.class_names)[det_class_ids]))\n",
    "\n",
    "captions = [\"{} {:.3f}\".format(dataset.class_names[int(c)], s) if c > 0 else \"\"\n",
    "            for c, s in zip(detections[:, 4], detections[:, 5])]\n",
    "visualize.draw_boxes(\n",
    "    image, \n",
    "    refined_boxes=utils.denorm_boxes(detections[:, :4], image.shape[:2]),\n",
    "    visibilities=[2] * len(detections),\n",
    "    captions=captions, title=\"Detections\",\n",
    "    ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c Step by Step Detection\n",
    "\n",
    "Here we dive deeper into the process of processing the detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Valid proposals out of 1000\n",
      "71 Positive ROIs\n",
      "[('BG', 929), ('airplane', 23), ('car', 11), ('person', 37)]\n"
     ]
    }
   ],
   "source": [
    "# Proposals are in normalized coordinates. Scale them\n",
    "# to image coordinates.\n",
    "h, w = config.IMAGE_SHAPE[:2]\n",
    "proposals = np.around(mrcnn[\"proposals\"][0] * np.array([h, w, h, w])).astype(np.int32)\n",
    "\n",
    "# Class ID, score, and mask per proposal\n",
    "roi_class_ids = np.argmax(mrcnn[\"probs\"][0], axis=1)\n",
    "roi_scores = mrcnn[\"probs\"][0, np.arange(roi_class_ids.shape[0]), roi_class_ids]\n",
    "roi_class_names = np.array(dataset.class_names)[roi_class_ids]\n",
    "roi_positive_ixs = np.where(roi_class_ids > 0)[0]\n",
    "\n",
    "# How many ROIs vs empty rows?\n",
    "print(\"{} Valid proposals out of {}\".format(np.sum(np.any(proposals, axis=1)), proposals.shape[0]))\n",
    "print(\"{} Positive ROIs\".format(len(roi_positive_ixs)))\n",
    "\n",
    "# Class counts\n",
    "print(list(zip(*np.unique(roi_class_names, return_counts=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of proposals.\n",
    "# Proposals classified as background are dotted, and\n",
    "# the rest show their class and confidence score.\n",
    "limit = 200\n",
    "ixs = np.random.randint(0, proposals.shape[0], limit)\n",
    "captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n",
    "            for c, s in zip(roi_class_ids[ixs], roi_scores[ixs])]\n",
    "visualize.draw_boxes(image, boxes=proposals[ixs],\n",
    "                     visibilities=np.where(roi_class_ids[ixs] > 0, 2, 1),\n",
    "                     captions=captions, title=\"ROIs Before Refinement\",\n",
    "                     ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Bounding Box Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-specific bounding box shifts.\n",
    "roi_bbox_specific = mrcnn[\"deltas\"][0, np.arange(proposals.shape[0]), roi_class_ids]\n",
    "log(\"roi_bbox_specific\", roi_bbox_specific)\n",
    "\n",
    "# Apply bounding box transformations\n",
    "# Shape: [N, (y1, x1, y2, x2)]\n",
    "refined_proposals = utils.apply_box_deltas(\n",
    "    proposals, roi_bbox_specific * config.BBOX_STD_DEV).astype(np.int32)\n",
    "log(\"refined_proposals\", refined_proposals)\n",
    "\n",
    "# Show positive proposals\n",
    "# ids = np.arange(roi_boxes.shape[0])  # Display all\n",
    "limit = 5\n",
    "ids = np.random.randint(0, len(roi_positive_ixs), limit)  # Display random sample\n",
    "captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n",
    "            for c, s in zip(roi_class_ids[roi_positive_ixs][ids], roi_scores[roi_positive_ixs][ids])]\n",
    "visualize.draw_boxes(image, boxes=proposals[roi_positive_ixs][ids],\n",
    "                     refined_boxes=refined_proposals[roi_positive_ixs][ids],\n",
    "                     visibilities=np.where(roi_class_ids[roi_positive_ixs][ids] > 0, 1, 0),\n",
    "                     captions=captions, title=\"ROIs After Refinement\",\n",
    "                     ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Low Confidence Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep 71 detections:\n",
      "[  0   2   3   4   5   6   8   9  16  17  18  19  25  30  36  37  38  40\n",
      "  42  50  51  67  68  74  78  79  92 158 162 177 187 191 209 225 261 284\n",
      " 292 314 328 374 402 403 409 429 473 490 499 516 545 557 572 575 607 624\n",
      " 638 639 671 703 719 744 753 754 778 790 813 815 848 862 865 876 911]\n"
     ]
    }
   ],
   "source": [
    "# Remove boxes classified as background\n",
    "keep = np.where(roi_class_ids > 0)[0]\n",
    "print(\"Keep {} detections:\\n{}\".format(keep.shape[0], keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove boxes below 0.5 confidence. Keep 67:\n",
      "[  0   2   4   5   6   8   9  16  17  18  19  25  30  36  37  38  40  42\n",
      "  50  51  67  68  74  78  79 158 162 177 187 191 209 225 284 292 314 328\n",
      " 374 402 403 409 429 473 490 499 516 545 557 575 607 624 638 639 671 703\n",
      " 719 744 753 754 778 790 813 815 848 862 865 876 911]\n"
     ]
    }
   ],
   "source": [
    "# Remove low confidence detections\n",
    "keep = np.intersect1d(keep, np.where(roi_scores >= config.DETECTION_MIN_CONFIDENCE)[0])\n",
    "print(\"Remove boxes below {} confidence. Keep {}:\\n{}\".format(\n",
    "    config.DETECTION_MIN_CONFIDENCE, keep.shape[0], keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per-Class Non-Max Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person                : [  0   2   5   6   9  67  68  74  79 158 162 187 191 225 284 374 403 409\n",
      " 429 490 545 557 575 607 638 671 703 744 753 754 778 790 813 848 862 876\n",
      " 911] -> [  0 162   9   2 671]\n",
      "car                   : [ 16  18  30  36  51 177 314 328 499 624 815] -> [30]\n",
      "airplane              : [  4   8  17  19  25  37  38  40  42  50  78 209 292 402 473 516 639 719\n",
      " 865] -> [78 19]\n",
      "\n",
      "Kept after per-class NMS: 8\n",
      "[  0   2   9  19  30  78 162 671]\n"
     ]
    }
   ],
   "source": [
    "# Apply per-class non-max suppression\n",
    "pre_nms_boxes = refined_proposals[keep]\n",
    "pre_nms_scores = roi_scores[keep]\n",
    "pre_nms_class_ids = roi_class_ids[keep]\n",
    "\n",
    "nms_keep = []\n",
    "for class_id in np.unique(pre_nms_class_ids):\n",
    "    # Pick detections of this class\n",
    "    ixs = np.where(pre_nms_class_ids == class_id)[0]\n",
    "    # Apply NMS\n",
    "    class_keep = utils.non_max_suppression(pre_nms_boxes[ixs], \n",
    "                                            pre_nms_scores[ixs],\n",
    "                                            config.DETECTION_NMS_THRESHOLD)\n",
    "    # Map indicies\n",
    "    class_keep = keep[ixs[class_keep]]\n",
    "    nms_keep = np.union1d(nms_keep, class_keep)\n",
    "    print(\"{:22}: {} -> {}\".format(dataset.class_names[class_id][:20], \n",
    "                                   keep[ixs], class_keep))\n",
    "\n",
    "keep = np.intersect1d(keep, nms_keep).astype(np.int32)\n",
    "print(\"\\nKept after per-class NMS: {}\\n{}\".format(keep.shape[0], keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final detections\n",
    "ixs = np.arange(len(keep))  # Display all\n",
    "# ixs = np.random.randint(0, len(keep), 10)  # Display random sample\n",
    "captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n",
    "            for c, s in zip(roi_class_ids[keep][ixs], roi_scores[keep][ixs])]\n",
    "visualize.draw_boxes(\n",
    "    image, boxes=proposals[keep][ixs],\n",
    "    refined_boxes=refined_proposals[keep][ixs],\n",
    "    visibilities=np.where(roi_class_ids[keep][ixs] > 0, 1, 0),\n",
    "    captions=captions, title=\"Detections after NMS\",\n",
    "    ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Generating Masks\n",
    "\n",
    "This stage takes the detections (refined bounding boxes and class IDs) from the previous layer and runs the mask head to generate segmentation masks for every instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Mask Targets\n",
    "\n",
    "These are the training targets for the mask branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(np.transpose(gt_mask, [2, 0, 1]), cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Predicted Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detections               shape: (1, 100, 6)           min:    0.00000  max:  844.00000\n",
      "masks                    shape: (1, 100, 28, 28, 81)  min:    0.00000  max:    0.99984\n",
      "8 detections: ['person' 'person' 'person' 'person' 'person' 'airplane' 'airplane' 'car']\n"
     ]
    }
   ],
   "source": [
    "# Get predictions of mask head\n",
    "mrcnn = model.run_graph([image], [\n",
    "    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n",
    "    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n",
    "])\n",
    "\n",
    "# Get detection class IDs. Trim zero padding.\n",
    "det_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\n",
    "det_count = np.where(det_class_ids == 0)[0][0]\n",
    "det_class_ids = det_class_ids[:det_count]\n",
    "\n",
    "print(\"{} detections: {}\".format(\n",
    "    det_count, np.array(dataset.class_names)[det_class_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det_mask_specific        shape: (8, 28, 28)           min:    0.00001  max:    0.99984\n",
      "det_masks                shape: (8, 1024, 1024)       min:    0.00000  max:    1.00000\n"
     ]
    }
   ],
   "source": [
    "# Masks\n",
    "det_boxes = utils.denorm_boxes(mrcnn[\"detections\"][0, :, :4], image.shape[:2])\n",
    "det_mask_specific = np.array([mrcnn[\"masks\"][0, i, :, :, c] \n",
    "                              for i, c in enumerate(det_class_ids)])\n",
    "det_masks = np.array([utils.unmold_mask(m, det_boxes[i], image.shape)\n",
    "                      for i, m in enumerate(det_mask_specific)])\n",
    "log(\"det_mask_specific\", det_mask_specific)\n",
    "log(\"det_masks\", det_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(det_mask_specific[:4] * 255, cmap=\"Blues\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(det_masks[:4] * 255, cmap=\"Blues\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Activations\n",
    "\n",
    "In some cases it helps to look at the output from different layers and visualize them to catch issues and odd patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_image              shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10001\n",
      "res4w_out                shape: (1, 64, 64, 1024)     min:    0.00000  max:   54.64681\n",
      "rpn_bbox                 shape: (1, 65472, 4)         min:  -12.26412  max:   18.18265\n",
      "roi                      shape: (1, 1000, 4)          min:    0.00000  max:    1.00000\n"
     ]
    }
   ],
   "source": [
    "# Get activations of a few sample layers\n",
    "activations = model.run_graph([image], [\n",
    "    (\"input_image\",        tf.identity(model.keras_model.get_layer(\"input_image\").output)),\n",
    "    (\"res4w_out\",          model.keras_model.get_layer(\"res4w_out\").output),  # for resnet100\n",
    "    (\"rpn_bbox\",           model.keras_model.get_layer(\"rpn_bbox\").output),\n",
    "    (\"roi\",                model.keras_model.get_layer(\"ROI\").output),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image (normalized)\n",
    "_ = plt.imshow(modellib.unmold_image(activations[\"input_image\"][0],config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backbone feature map\n",
    "display_images(np.transpose(activations[\"res4w_out\"][0,:,:,:4], [2, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of RPN bounding box deltas\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title(\"dy\")\n",
    "_ = plt.hist(activations[\"rpn_bbox\"][0,:,0], 50)\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title(\"dx\")\n",
    "_ = plt.hist(activations[\"rpn_bbox\"][0,:,1], 50)\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title(\"dw\")\n",
    "_ = plt.hist(activations[\"rpn_bbox\"][0,:,2], 50)\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.title(\"dh\")\n",
    "_ = plt.hist(activations[\"rpn_bbox\"][0,:,3], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of y, x coordinates of generated proposals\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"y1, x1\")\n",
    "plt.scatter(activations[\"roi\"][0,:,0], activations[\"roi\"][0,:,1])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"y2, x2\")\n",
    "plt.scatter(activations[\"roi\"][0,:,2], activations[\"roi\"][0,:,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
