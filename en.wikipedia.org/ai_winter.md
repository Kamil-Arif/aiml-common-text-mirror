In the history of artificial intelligence, an AI winter is a period of
reduced funding and interest in artificial intelligence research. The
term was coined by analogy to the idea of a nuclear winter. The field
has experienced several hype cycles, followed by disappointment and
criticism, followed by funding cuts, followed by renewed interest years
or even decades later. The term first appeared in 1984 as the topic of a
public debate at the annual meeting of AAAI (then called the \"American
Association of Artificial Intelligence\"). It is a chain reaction that
begins with pessimism in the AI community, followed by pessimism in the
press, followed by a severe cutback in funding, followed by the end of
serious research. At the meeting, Roger Schank and Marvin Minsky---two
leading AI researchers who had survived the \"winter\" of the
1970s---warned the business community that enthusiasm for AI had
spiraled out of control in the 1980s and that disappointment would
certainly follow. Three years later, the billion-dollar AI industry
began to collapse.Hype is common in many emerging technologies, such as
the railway mania or the dot-com bubble. The AI winter was a result of
such hype, due to over-inflated promises by developers, unnaturally high
expectations from end-users, and extensive promotion in the media.
Despite the rise and fall of AI\'s reputation, it has continued to
develop new and successful technologies. AI researcher Rodney Brooks
would complain in 2002 that \"there\'s this stupid myth out there that
AI has failed, but AI is around you every second of the day.\" In 2005,
Ray Kurzweil agreed: \"Many observers still think that the AI winter was
the end of the story and that nothing since has come of the AI field.
Yet today many thousands of AI applications are deeply embedded in the
infrastructure of every industry.\"Enthusiasm and optimism about AI has
generally increased since its low point in the early 1990s. Beginning
about 2012, interest in artificial intelligence (and especially the
sub-field of machine learning) from the research and corporate
communities led to a dramatic increase in funding and investment,
leading to the current (as of 2023) AI boom. Quantum winter is the
prospect of a similar development in quantum computing, anticipated or
contemplated by Mikhail Dyakonov, Chris Hoofnagle, Simson Garfinkel,
Victor Galitsky, and Nikita Gourianov. Overview There were two major
winters in 1974--1980 and 1987--1993 and several smaller episodes,
including the following: 1966: failure of machine translation 1970:
abandonment of connectionism Period of overlapping trends: 1971--75:
DARPA\'s frustration with the Speech Understanding Research program at
Carnegie Mellon University 1973: large decrease in AI research in the
United Kingdom in response to the Lighthill report 1973--74: DARPA\'s
cutbacks to academic AI research in general 1987: collapse of the LISP
machine market 1988: cancellation of new spending on AI by the Strategic
Computing Initiative 1993: resistance to new expert systems deployment
and maintenance 1990s: end of the Fifth Generation computer project\'s
original goals Early episodes Machine translation and the ALPAC report
of 1966 During the Cold War, the US government was particularly
interested in the automatic, instant translation of Russian documents
and scientific reports. The government aggressively supported efforts at
machine translation starting in 1954. At the outset, the researchers
were optimistic. Noam Chomsky\'s new work in grammar was streamlining
the translation process and there were \"many predictions of imminent
\'breakthroughs\'\". However, researchers had underestimated the
profound difficulty of word-sense disambiguation. In order to translate
a sentence, a machine needed to have some idea what the sentence was
about, otherwise it made mistakes. An apocryphal example is \"the spirit
is willing but the flesh is weak.\" Translated back and forth with
Russian, it became \"the vodka is good but the meat is rotten.\" Later
researchers would call this the commonsense knowledge problem. By 1964,
the National Research Council had become concerned about the lack of
progress and formed the Automatic Language Processing Advisory Committee
(ALPAC) to look into the problem. They concluded, in a famous 1966
report, that machine translation was more expensive, less accurate and
slower than human translation. After spending some 20 million dollars,
the NRC ended all support. Careers were destroyed and research
ended.Machine translation is still an open research problem in the 21st
century, which has met with some success (Google Translate, Yahoo Babel
Fish). The abandonment of connectionism in 1969 Some of the earliest
work in AI used networks or circuits of connected units to simulate
intelligent behavior. Examples of this kind of work, called
\"connectionism\", include Walter Pitts and Warren McCulloch\'s first
description of a neural network for logic and Marvin Minsky\'s work on
the SNARC system. In the late 1950s, most of these approaches were
abandoned when researchers began to explore symbolic reasoning as the
essence of intelligence, following the success of programs like the
Logic Theorist and the General Problem Solver.However, one type of
connectionist work continued: the study of perceptrons, invented by
Frank Rosenblatt, who kept the field alive with his salesmanship and the
sheer force of his personality. He optimistically predicted that the
perceptron \"may eventually be able to learn, make decisions, and
translate languages\". Mainstream research into perceptrons came to an
abrupt end in 1969, when Marvin Minsky and Seymour Papert published the
book Perceptrons, which was perceived as outlining the limits of what
perceptrons could do. Connectionist approaches were abandoned for the
next decade or so. While important work, such as Paul Werbos\' discovery
of backpropagation, continued in a limited way, major funding for
connectionist projects was difficult to find in the 1970s and early
1980s. The \"winter\" of connectionist research came to an end in the
middle 1980s, when the work of John Hopfield, David Rumelhart and others
revived large scale interest in neural networks. Rosenblatt did not live
to see this, however, as he died in a boating accident shortly after
Perceptrons was published. The setbacks of 1974 The Lighthill report In
1973, professor Sir James Lighthill was asked by the UK Parliament to
evaluate the state of AI research in the United Kingdom. His report, now
called the Lighthill report, criticized the utter failure of AI to
achieve its \"grandiose objectives\". He concluded that nothing being
done in AI could not be done in other sciences. He specifically
mentioned the problem of \"combinatorial explosion\" or
\"intractability\", which implied that many of AI\'s most successful
algorithms would grind to a halt on real world problems and were only
suitable for solving \"toy\" versions.The report was contested in a
debate broadcast in the BBC \"Controversy\" series in 1973. The debate
\"The general purpose robot is a mirage\" from the Royal Institution was
Lighthill versus the team of Donald Michie, John McCarthy and Richard
Gregory. McCarthy later wrote that \"the combinatorial explosion problem
has been recognized in AI from the beginning\".The report led to the
complete dismantling of AI research in England. AI research continued in
only a few universities (Edinburgh, Essex and Sussex). Research would
not revive on a large scale until 1983, when Alvey (a research project
of the British Government) began to fund AI again from a war chest of
Â£350 million in response to the Japanese Fifth Generation Project (see
below). Alvey had a number of UK-only requirements which did not sit
well internationally, especially with US partners, and lost Phase 2
funding. DARPA\'s early 1970s funding cuts During the 1960s, the Defense
Advanced Research Projects Agency (then known as \"ARPA\", now known as
\"DARPA\") provided millions of dollars for AI research with few strings
attached. J. C. R. Licklider, the founding director of DARPA\'s
computing division, believed in \"funding people, not projects\" and he
and several successors allowed AI\'s leaders (such as Marvin Minsky,
John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any
way they liked. This attitude changed after the passage of Mansfield
Amendment in 1969, which required DARPA to fund \"mission-oriented
direct research, rather than basic undirected research\". Pure
undirected research of the kind that had gone on in the 1960s would no
longer be funded by DARPA. Researchers now had to show that their work
would soon produce some useful military technology. AI research
proposals were held to a very high standard. The situation was not
helped when the Lighthill report and DARPA\'s own study (the American
Study Group) suggested that most AI research was unlikely to produce
anything truly useful in the foreseeable future. DARPA\'s money was
directed at specific projects with identifiable goals, such as
autonomous tanks and battle management systems. By 1974, funding for AI
projects was hard to find.AI researcher Hans Moravec blamed the crisis
on the unrealistic predictions of his colleagues: \"Many researchers
were caught up in a web of increasing exaggeration. Their initial
promises to DARPA had been much too optimistic. Of course, what they
delivered stopped considerably short of that. But they felt they
couldn\'t in their next proposal promise less than in the first one, so
they promised more.\" The result, Moravec claims, is that some of the
staff at DARPA had lost patience with AI research. \"It was literally
phrased at DARPA that \'some of these people were going to be taught a
lesson \[by\] having their two-million-dollar-a-year contracts cut to
almost nothing!\'\" Moravec told Daniel Crevier.While the autonomous
tank project was a failure, the battle management system (the Dynamic
Analysis and Replanning Tool) proved to be enormously successful, saving
billions in the first Gulf War, repaying all of DARPAs investment in AI
and justifying DARPA\'s pragmatic policy. The SUR debacle DARPA was
deeply disappointed with researchers working on the Speech Understanding
Research program at Carnegie Mellon University. DARPA had hoped for, and
felt it had been promised, a system that could respond to voice commands
from a pilot. The SUR team had developed a system which could recognize
spoken English, but only if the words were spoken in a particular order.
DARPA felt it had been duped and, in 1974, they cancelled a three
million dollar a year contract.Many years later, several successful
commercial speech recognition systems would use the technology developed
by the Carnegie Mellon team (such as hidden Markov models) and the
market for speech recognition systems would reach \$4 billion by 2001.
The setbacks of the late 1980s and early 1990s The collapse of the LISP
machine market In the 1980s, a form of AI program called an \"expert
system\" was adopted by corporations around the world. The first
commercial expert system was XCON, developed at Carnegie Mellon for
Digital Equipment Corporation, and it was an enormous success: it was
estimated to have saved the company 40 million dollars over just six
years of operation. Corporations around the world began to develop and
deploy expert systems and by 1985 they were spending over a billion
dollars on AI, most of it to in-house AI departments. An industry grew
up to support them, including software companies like Teknowledge and
Intellicorp (KEE), and hardware companies like Symbolics and LISP
Machines Inc. who built specialized computers, called LISP machines,
that were optimized to process the programming language LISP, the
preferred language for AI.In 1987, three years after Minsky and
Schank\'s prediction, the market for specialized LISP-based AI hardware
collapsed. Workstations by companies like Sun Microsystems offered a
powerful alternative to LISP machines and companies like Lucid offered a
LISP environment for this new class of workstations. The performance of
these general workstations became an increasingly difficult challenge
for LISP Machines. Companies like Lucid and Franz LISP offered
increasingly powerful versions of LISP that were portable to all UNIX
systems. For example, benchmarks were published showing workstations
maintaining a performance advantage over LISP machines. Later desktop
computers built by Apple and IBM would also offer a simpler and more
popular architecture to run LISP applications on. By 1987, some of them
had become as powerful as the more expensive LISP machines. The desktop
computers had rule-based engines such as CLIPS available. These
alternatives left consumers with no reason to buy an expensive machine
specialized for running LISP. An entire industry worth half a billion
dollars was replaced in a single year.By the early 1990s, most
commercial LISP companies had failed, including Symbolics, LISP Machines
Inc., Lucid Inc., etc. Other companies, like Texas Instruments and
Xerox, abandoned the field. A small number of customer companies (that
is, companies using systems written in LISP and developed on LISP
machine platforms) continued to maintain systems. In some cases, this
maintenance involved the assumption of the resulting support work.
Slowdown in deployment of expert systems By the early 1990s, the
earliest successful expert systems, such as XCON, proved too expensive
to maintain. They were difficult to update, they could not learn, they
were \"brittle\" (i.e., they could make grotesque mistakes when given
unusual inputs), and they fell prey to problems (such as the
qualification problem) that had been identified years earlier in
research in nonmonotonic logic. Expert systems proved useful, but only
in a few special contexts. Another problem dealt with the computational
hardness of truth maintenance efforts for general knowledge. KEE used an
assumption-based approach (see NASA, TEXSYS) supporting multiple-world
scenarios that was difficult to understand and apply. The few remaining
expert system shell companies were eventually forced to downsize and
search for new markets and software paradigms, like case-based reasoning
or universal database access. The maturation of Common Lisp saved many
systems such as ICAD which found application in knowledge-based
engineering. Other systems, such as Intellicorp\'s KEE, moved from LISP
to a C++ (variant) on the PC and helped establish object-oriented
technology (including providing major support for the development of UML
(see UML Partners). The end of the Fifth Generation project In 1981, the
Japanese Ministry of International Trade and Industry set aside \$850
million for the Fifth Generation computer project. Their objectives were
to write programs and build machines that could carry on conversations,
translate languages, interpret pictures, and reason like human beings.
By 1991, the impressive list of goals penned in 1981 had not been met.
According to HP Newquist in The Brain Makers, \"On June 1, 1992, The
Fifth Generation Project ended not with a successful roar, but with a
whimper.\" As with other AI projects, expectations had run much higher
than what was actually possible. Strategic Computing Initiative cutbacks
In 1983, in response to the fifth generation project, DARPA again began
to fund AI research through the Strategic Computing Initiative. As
originally proposed the project would begin with practical, achievable
goals, which even included artificial general intelligence as long-term
objective. The program was under the direction of the Information
Processing Technology Office (IPTO) and was also directed at
supercomputing and microelectronics. By 1985 it had spent \$100 million
and 92 projects were underway at 60 institutions, half in industry, half
in universities and government labs. AI research was well-funded by the
SCI.Jack Schwarz, who ascended to the leadership of IPTO in 1987,
dismissed expert systems as \"clever programming\" and cut funding to AI
\"deeply and brutally\", \"eviscerating\" SCI. Schwarz felt that DARPA
should focus its funding only on those technologies which showed the
most promise, in his words, DARPA should \"surf\", rather than \"dog
paddle\", and he felt strongly AI was not \"the next wave\". Insiders in
the program cited problems in communication, organization and
integration. A few projects survived the funding cuts, including
pilot\'s assistant and an autonomous land vehicle (which were never
delivered) and the DART battle management system, which (as noted above)
was successful. Developments post-AI winter A survey of reports from the
early 2000s suggests that AI\'s reputation was still less than stellar:
Alex Castro, quoted in The Economist, 7 June 2007: \"\[Investors\] were
put off by the term \'voice recognition\' which, like \'artificial
intelligence\', is associated with systems that have all too often
failed to live up to their promises.\" Patty Tascarella in Pittsburgh
Business Times, 2006: \"Some believe the word \'robotics\' actually
carries a stigma that hurts a company\'s chances at funding.\" John
Markoff in the New York Times, 2005: \"At its low point, some computer
scientists and software engineers avoided the term artificial
intelligence for fear of being viewed as wild-eyed dreamers.\"Many
researchers in AI in the mid 2000s deliberately called their work by
other names, such as informatics, machine learning, analytics,
knowledge-based systems, business rules management, cognitive systems,
intelligent systems, intelligent agents or computational intelligence,
to indicate that their work emphasizes particular tools or is directed
at a particular sub-problem. Although this may be partly because they
consider their field to be fundamentally different from AI, it is also
true that the new names help to procure funding by avoiding the stigma
of false promises attached to the name \"artificial intelligence\". AI
integration In the late 1990s and early 21st century, AI technology
became widely used as elements of larger systems, but the field is
rarely credited for these successes. In 2006, Nick Bostrom explained
that \"a lot of cutting edge AI has filtered into general applications,
often without being called AI because once something becomes useful
enough and common enough it\'s not labeled AI anymore.\" Rodney Brooks
stated around the same time that \"there\'s this stupid myth out there
that AI has failed, but AI is around you every second of the
day.\"Technologies developed by AI researchers have achieved commercial
success in a number of domains, such as machine translation, data
mining, industrial robotics, logistics, speech recognition, banking
software, medical diagnosis, and Google\'s search engine.Fuzzy logic
controllers have been developed for automatic gearboxes in automobiles
(the 2006 Audi TT, VW Touareg and VW Caravelle feature the DSP
transmission which utilizes fuzzy logic, a number of Å koda variants
(Å koda Fabia) also currently include a fuzzy logic-based controller).
Camera sensors widely utilize fuzzy logic to enable focus. Heuristic
search and data analytics are both technologies that have developed from
the evolutionary computing and machine learning subdivision of the AI
research community. Again, these techniques have been applied to a wide
range of real world problems with considerable commercial success. Data
analytics technology utilizing algorithms for the automated formation of
classifiers that were developed in the supervised machine learning
community in the 1990s (for example, TDIDT, Support Vector Machines,
Neural Nets, IBL) are now used pervasively by companies for marketing
survey targeting and discovery of trends and features in data sets. AI
funding Researchers and economists frequently judged the status of an AI
winter by reviewing which AI projects were being funded, how much and by
whom. Trends in funding are often set by major funding agencies in the
developed world. Currently, DARPA and a civilian funding program called
EU-FP7 provide much of the funding for AI research in the US and
European Union. As of 2007, DARPA was soliciting AI research proposals
under a number of programs including The Grand Challenge Program,
Cognitive Technology Threat Warning System (CT2WS), \"Human Assisted
Neural Devices (SN07-43)\", \"Autonomous Real-Time Ground Ubiquitous
Surveillance-Imaging System (ARGUS-IS)\" and \"Urban Reasoning and
Geospatial Exploitation Technology (URGENT)\" Perhaps best known is
DARPA\'s Grand Challenge Program which has developed fully automated
road vehicles that can successfully navigate real world terrain in a
fully autonomous fashion. DARPA has also supported programs on the
Semantic Web with a great deal of emphasis on intelligent management of
content and automated understanding. However James Hendler, the manager
of the DARPA program at the time, expressed some disappointment with the
government\'s ability to create rapid change, and moved to working with
the World Wide Web Consortium to transition the technologies to the
private sector. The EU-FP7 funding program provides financial support to
researchers within the European Union. In 2007--2008, it was funding AI
research under the Cognitive Systems: Interaction and Robotics Programme
(â¬193m), the Digital Libraries and Content Programme (â¬203m) and the FET
programme (â¬185m). Current \"AI spring\" A marked increase in AI
funding, development, deployment, and commercial use has led to the idea
of the AI winter being long over. Concerns are occasionally raised that
a new AI winter could be triggered by overly ambitious or unrealistic
promises by prominent AI scientists or overpromising on the part of
commercial vendors. The successes of the current \"AI spring\" or \"AI
boom\" are advances in language translation (in particular, Google
Translate), image recognition (spurred by the ImageNet training
database) as commercialized by Google Image Search, and in game-playing
systems such as AlphaZero (chess champion) and AlphaGo (go champion),
and Watson (Jeopardy champion). Most of these advances have occurred
since 2010. Underlying causes behind AI winters Several explanations
have been put forth for the cause of AI winters in general. As AI
progressed from government-funded applications to commercial ones, new
dynamics came into play. While hype is the most commonly cited cause,
the explanations are not necessarily mutually exclusive. Hype The AI
winters can be partly understood as a sequence of over-inflated
expectations and subsequent crash seen in stock-markets and exemplified
by the railway mania and dotcom bubble. In a common pattern in the
development of new technology (known as hype cycle), an event, typically
a technological breakthrough, creates publicity which feeds on itself to
create a \"peak of inflated expectations\" followed by a \"trough of
disillusionment\". Since scientific and technological progress cannot
keep pace with the publicity-fueled increase in expectations among
investors and other stakeholders, a crash must follow. AI technology
seems to be no exception to this rule.For example, in the 1960s the
realization that computers could simulate 1-layer neural networks led to
a neural-network hype cycle that lasted until the 1969 publication of
the book Perceptrons which severely limited the set of problems that
could be optimally solved by 1-layer networks. In 1985 the realization
that neural networks could be used to solve optimization problems, as a
result of famous papers by Hopfield and Tank, together with the threat
of Japan\'s fifth-generation project, led to renewed interest and
application. Institutional factors Another factor is AI\'s place in the
organisation of universities. Research on AI often takes the form of
interdisciplinary research. AI is therefore prone to the same problems
other types of interdisciplinary research face. Funding is channeled
through the established departments and during budget cuts, there will
be a tendency to shield the \"core contents\" of each department, at the
expense of interdisciplinary and less traditional research projects.
Economic factors Downturns in a country\'s national economy cause budget
cuts in universities. The \"core contents\" tendency worsens the effect
on AI research and investors in the market are likely to put their money
into less risky ventures during a crisis. Together this may amplify an
economic downturn into an AI winter. It is worth noting that the
Lighthill report came at a time of economic crisis in the UK, when
universities had to make cuts and the question was only which programs
should go. Insufficient computing capability Early in the computing
history the potential for neural networks was understood but it has
never been realized. Fairly simple networks require significant
computing capacity even by today\'s standards. Empty pipeline It is
common to see the relationship between basic research and technology as
a pipeline. Advances in basic research give birth to advances in applied
research, which in turn leads to new commercial applications. From this
it is often argued that a lack of basic research will lead to a drop in
marketable technology some years down the line. This view was advanced
by James Hendler in 2008, when he claimed that the fall of expert
systems in the late \'80s was not due to an inherent and unavoidable
brittleness of expert systems, but to funding cuts in basic research in
the 1970s. These expert systems advanced in the 1980s through applied
research and product development, but, by the end of the decade, the
pipeline had run dry and expert systems were unable to produce
improvements that could have overcome this brittleness and secured
further funding. Failure to adapt The fall of the LISP machine market
and the failure of the fifth generation computers were cases of
expensive advanced products being overtaken by simpler and cheaper
alternatives. This fits the definition of a low-end disruptive
technology, with the LISP machine makers being marginalized. Expert
systems were carried over to the new desktop computers by for instance
CLIPS, so the fall of the LISP machine market and the fall of expert
systems are strictly speaking two separate events. Still, the failure to
adapt to such a change in the outside computing milieu is cited as one
reason for the 1980s AI winter. Arguments and debates on past and future
of AI Several philosophers, cognitive scientists and computer scientists
have speculated on where AI might have failed and what lies in its
future. Hubert Dreyfus highlighted flawed assumptions of AI research in
the past and, as early as 1966, correctly predicted that the first wave
of AI research would fail to fulfill the very public promises it was
making. Other critics like Noam Chomsky have argued that AI is headed in
the wrong direction, in part because of its heavy reliance on
statistical techniques. Chomsky\'s comments fit into a larger debate
with Peter Norvig, centered around the role of statistical methods in
AI. The exchange between the two started with comments made by Chomsky
at a symposium at MIT to which Norvig wrote a response. See also History
of artificial neural networks History of artificial intelligence AI
effect Software crisis Notes References Crevier, Daniel (1993). AI: The
Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks.
ISBN 0-465-02997-3. Hendler, James (2007). \"Where Are All the
Intelligent Agents?\". IEEE Intelligent Systems. 22 (3): 2--3.
doi:10.1109/MIS.2007.62. Howe, J. (November 1994). \"Artificial
Intelligence at Edinburgh University : a Perspective\". Archived from
the original on 17 August 2007. Retrieved 30 August 2007. Kaplan,
Andreas; Haenlein, Michael (2018). \"Siri, Siri in my Hand, who\'s the
Fairest in the Land? On the Interpretations, Illustrations and
Implications of Artificial Intelligence\". Business Horizons. Business
Horizons 62(1). 62: 15--25. doi:10.1016/j.bushor.2018.08.004. S2CID
158433736. Kurzweil, Ray (2005). \"The Singularity is Near\". Viking
Press. {{cite journal}}: Cite journal requires \|journal= (help)
Lighthill, Professor Sir James (1973). \"Artificial Intelligence: A
General Survey\". Artificial Intelligence: a paper symposium. Science
Research Council.Minsky, Marvin; Papert, Seymour (1969). \"Perceptrons:
An Introduction to Computational Geometry\". The MIT Press. {{cite
journal}}: Cite journal requires \|journal= (help) McCorduck, Pamela
(2004), Machines Who Think (2nd ed.), Natick, MA: A. K. Peters, Ltd.,
ISBN 1-56881-205-1 NRC (1999). \"Developments in Artificial
Intelligence\". Funding a Revolution: Government Support for Computing
Research. National Academy Press. Archived from the original on 12
January 2008. Retrieved 30 August 2007.{{cite book}}: CS1 maint: bot:
original URL status unknown (link) Newquist, HP (1994). The Brain
Makers: Genius, Ego, and Greed In The Search For Machines That Think.
Macmillan/SAMS. ISBN 978-0-9885937-1-8. Russell, Stuart J.; Norvig,
Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.),
Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2
Further reading Marcus, Gary, \"Am I Human?: Researchers need new ways
to distinguish artificial intelligence from the natural kind\",
Scientific American, vol. 316, no. 3 (March 2017), pp. 58--63. Multiple
tests of artificial-intelligence efficacy are needed because, \"just as
there is no single test of athletic prowess, there cannot be one
ultimate test of intelligence.\" One such test, a \"Construction
Challenge\", would test perception and physical action---\"two important
elements of intelligent behavior that were entirely absent from the
original Turing test.\" Another proposal has been to give machines the
same standardized tests of science and other disciplines that
schoolchildren take. A so far insuperable stumbling block to artificial
intelligence is an incapacity for reliable disambiguation.
\"\[V\]irtually every sentence \[that people generate\] is ambiguous,
often in multiple ways.\" A prominent example is known as the \"pronoun
disambiguation problem\": a machine has no way of determining to whom or
what a pronoun in a sentence---such as \"he\", \"she\" or
\"it\"---refers. Luke Muehlhauser (September 2016). \"What should we
learn from past AI forecasts?\". Open Philanthropy Project. External
links ComputerWorld article (February 2005) AI Expert Newsletter
(January 2005) \"If It Works, It\'s Not AI: A Commercial Look at
Artificial Intelligence startups\" Patterns of Software- a collection of
essays by Richard P. Gabriel, including several autobiographical essays
Review of \"Artificial Intelligence: A General Survey\" by John McCarthy
Other Freddy II Robot Resources Includes a link to the 90 minute 1973
\"Controversy\" debate from the Royal Academy of Lighthill vs. Michie,
McCarthy and Gregory in response to Lighthill\'s report to the British
government.
