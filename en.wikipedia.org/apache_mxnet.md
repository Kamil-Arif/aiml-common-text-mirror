Apache MXNet is an open-source deep learning software framework, used to
train and deploy deep neural networks. It is scalable, allowing for fast
model training and supports a flexible programming model and multiple
programming languages (including C++, Python, Java, Julia, MATLAB,
JavaScript, Go, R, Scala, Perl, and Wolfram Language). The MXNet library
is portable and can scale to multiple GPUs as well as multiple machines.
It was co-developed by Carlos Guestrin at University of Washington
(along with GraphLab). Features Apache MXNet is a scalable deep learning
framework that supports deep learning models, such as: convolutional
neural networks (CNNs) and long short-term memory networks (LSTMs).
Scalable MXNet can be distributed on dynamic cloud infrastructure using
a distributed parameter server (based on research at Carnegie Mellon
University, Baidu, and Google). With multiple GPUs or CPUs the framework
approaches linear scale. Flexible MXNet supports both imperative and
symbolic programming. The framework allows developers to track, debug,
save checkpoints, modify hyperparameters, and perform early stopping.
Multiple languages MXNet supports Python, R, Scala, Clojure, Julia,
Perl, MATLAB and JavaScript for front-end development, and C++ for
back-end optimization. Portable Supports an efficient deployment of a
trained model to low-end devices for inference, such as mobile devices
(using Amalgamation), Internet of things devices (using AWS Greengrass),
serverless computing (using AWS Lambda) or containers. These low-end
environments can have only weaker CPU or limited memory (RAM), and
should be able to use the models that were trained on a higher-level
environment (GPU based cluster, for example). Cloud Support MXNet is
supported by public cloud providers including Amazon Web Services (AWS)
and Microsoft Azure. Amazon has chosen MXNet as its deep learning
framework of choice at AWS. Currently, MXNet is supported by Intel,
Baidu, Microsoft, Wolfram Research, and research institutions such as
Carnegie Mellon, MIT, the University of Washington, and the Hong Kong
University of Science and Technology. See also Comparison of deep
learning software Differentiable programming == References ==
