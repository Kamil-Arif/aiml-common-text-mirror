DALL-E (stylized as DALL·E) and DALL-E 2 are deep learning models
developed by OpenAI to generate digital images from natural language
descriptions, called \"prompts\". DALL-E was revealed by OpenAI in a
blog post in January 2021, and uses a version of GPT-3 modified to
generate images. In April 2022, OpenAI announced DALL-E 2, a successor
designed to generate more realistic images at higher resolutions that
\"can combine concepts, attributes, and styles\".OpenAI has not released
source code for either model. On 20 July 2022, DALL-E 2 entered into a
beta phase with invitations sent to 1 million waitlisted individuals;
users can generate a certain number of images for free every month and
may purchase more. Access had previously been restricted to pre-selected
users for a research preview due to concerns about ethics and safety. On
28 September 2022, DALL-E 2 was opened to anyone and the waitlist
requirement was removed.In early November 2022, OpenAI released DALL-E 2
as an API, allowing developers to integrate the model into their own
applications. Microsoft unveiled their implementation of DALL-E 2 in
their Designer app and Image Creator tool included in Bing and Microsoft
Edge. CALA and Mixtiles are among other early adopters of the DALL-E 2
API. The API operates on a cost per image basis, with prices varying
depending on image resolution. Volume discounts are available to
companies working with OpenAI's enterprise team.The software\'s name is
a portmanteau of the names of animated robot Pixar character WALL-E and
the Spanish surrealist artist Salvador Dalí. Technology The first
generative pre-trained transformer (GPT) model was initially developed
by OpenAI in 2018, using a Transformer architecture. The first
iteration, GPT-1, was scaled up to produce GPT-2 in 2019; in 2020 it was
scaled up again to produce GPT-3, with 175 billion parameters. DALL-E\'s
model is a multimodal implementation of GPT-3 with 12 billion parameters
which \"swaps text for pixels\", trained on text-image pairs from the
Internet. DALL-E 2 uses 3.5 billion parameters, a smaller number than
its predecessor.DALL-E was developed and announced to the public in
conjunction with CLIP (Contrastive Language-Image Pre-training). CLIP is
a separate model based on zero-shot learning that was trained on 400
million pairs of images with text captions scraped from the Internet.
Its role is to \"understand and rank\" DALL-E\'s output by predicting
which caption from a list of 32,768 captions randomly selected from the
dataset (of which one was the correct answer) is most appropriate for an
image. This model is used to filter a larger initial list of images
generated by DALL-E to select the most appropriate outputs.DALL-E 2 uses
a diffusion model conditioned on CLIP image embeddings, which, during
inference, are generated from CLIP text embeddings by a prior model.
Capabilities DALL-E can generate imagery in multiple styles, including
photorealistic imagery, paintings, and emoji. It can \"manipulate and
rearrange\" objects in its images, and can correctly place design
elements in novel compositions without explicit instruction. Thom Dunn
writing for BoingBoing remarked that \"For example, when asked to draw a
daikon radish blowing its nose, sipping a latte, or riding a unicycle,
DALL-E often draws the handkerchief, hands, and feet in plausible
locations.\" DALL-E showed the ability to \"fill in the blanks\" to
infer appropriate details without specific prompts, such as adding
Christmas imagery to prompts commonly associated with the celebration,
and appropriately-placed shadows to images that did not mention them.
Furthermore, DALL-E exhibits a broad understanding of visual and design
trends.DALL-E can produce images for a wide variety of arbitrary
descriptions from various viewpoints with only rare failures. Mark
Riedl, an associate professor at the Georgia Tech School of Interactive
Computing, found that DALL-E could blend concepts (described as a key
element of human creativity).Its visual reasoning ability is sufficient
to solve Raven\'s Matrices (visual tests often administered to humans to
measure intelligence). Given an existing image, DALL-E 2 can produce
\"variations\" of the image as individual outputs based on the original
and edit the image to modify or expand upon it. DALL-E 2\'s
\"inpainting\" and \"outpainting\" use context from an image to fill in
missing areas using a medium consistent with the original, following a
given prompt. For example, this can be used to insert a new subject into
an image, or expand an image beyond its original borders. According to
OpenAI, \"Outpainting takes into account the image's existing visual
elements --- including shadows, reflections, and textures --- to
maintain the context of the original image.\" Ethical concerns DALL-E
2\'s reliance on public datasets influences its results and leads to
algorithmic bias in some cases, such as generating higher numbers of men
than women for requests that do not mention gender. DALL-E 2\'s training
data was filtered to remove violent and sexual imagery, but this was
found to increase bias in some cases such as reducing the frequency of
women being generated. OpenAI hypothesize that this may be because women
were more likely to be sexualized in training data which caused the
filter to influence results. In September 2022, OpenAI confirmed to The
Verge that DALL-E invisibly inserts phrases into user prompts to address
bias in results; for instance, \"black man\" and \"Asian woman\" are
inserted into prompts that do not specify gender or race.A concern about
DALL-E 2 and similar image generation models is that they could be used
to propagate deepfakes and other forms of misinformation. As an attempt
to mitigate this, the software rejects prompts involving public figures
and uploads containing human faces. Prompts containing potentially
objectionable content are blocked, and uploaded images are analyzed to
detect offensive material. A disadvantage of prompt-based filtering is
that it is easy to bypass using alternative phrases that result in a
similar output. For example, the word \"blood\" is filtered, but
\"ketchup\" and \"red liquid\" are not.Another concern about DALL-E 2
and similar models is that they could cause technological unemployment
for artists, photographers, and graphic designers due to their accuracy
and popularity. Technical limitations DALL-E 2\'s language understanding
has limits. It is sometimes unable to distinguish \"A yellow book and a
red vase\" from \"A red book and a yellow vase\" or \"A panda making
latte art\" from \"Latte art of a panda\". It generates images of \"an
astronaut riding a horse\" when presented with the prompt \"a horse
riding an astronaut\". It also fails to generate the correct images in a
variety of circumstances. Requesting more than three objects, negation,
numbers, and connected sentences may result in mistakes, and object
features may appear on the wrong object. Additional limitations include
handling text - which, even with legible lettering, almost invariably
results in dream-like gibberish - and its limited capacity to address
scientific information, such as astronomy or medical imagery. Reception
Most coverage of DALL-E focuses on a small subset of \"surreal\" or
\"quirky\" outputs. DALL-E\'s output for \"an illustration of a baby
daikon radish in a tutu walking a dog\" was mentioned in pieces from
Input, NBC, Nature, and other publications. Its output for \"an armchair
in the shape of an avocado\" was also widely covered.ExtremeTech stated
\"you can ask DALL-E for a picture of a phone or vacuum cleaner from a
specified period of time, and it understands how those objects have
changed\". Engadget also noted its unusual capacity for \"understanding
how telephones and other objects change over time\".According to MIT
Technology Review, one of OpenAI\'s objectives was to \"give language
models a better grasp of the everyday concepts that humans use to make
sense of things\".Wall Street investors have had a positive reception of
DALL-E 2, with some firms thinking it could represent a turning point
for a future multi-trillion dollar industry. OpenAI has already received
over \$1 billion in funding from Microsoft and Khosla Ventures.Japan\'s
anime community has had a negative reaction to DALL-E 2 and similar
models. Two arguments are typically presented by artists against the
software. The first is that AI art is not art because it is not created
by a human with intent. \"The juxtaposition of AI-generated images with
their own work is degrading and undermines the time and skill that goes
into their art. AI-driven image generation tools have been heavily
criticized by artists because they are trained on human-made art scraped
from the web.\" The second is the trouble with copyright law and data
text-to-image models are trained on. OpenAI has not released information
about what dataset(s) were used to train DALL-E 2, inciting concern from
some that the work of artists has been used for training without
permission. Copyright laws surrounding these topics are inconclusive at
the moment. Open-source implementations There have been several attempts
to create open-source implementations of DALL-E. Released in 2022 on
Hugging Face\'s Spaces platform, Craiyon (formerly DALL-E Mini until a
name change was requested by OpenAI in June 2022) is an AI model based
on the original DALL-E that was trained on unfiltered data from the
Internet. It attracted substantial media attention in mid-2022, after
its release due to its capacity for producing humorous imagery. See also
15.ai Artificial intelligence art Crungus DeepDream Imagen (Google
Brain) Midjourney Stable Diffusion Prompt engineering Synthography
References External links DALL E 2 website Craiyon website
