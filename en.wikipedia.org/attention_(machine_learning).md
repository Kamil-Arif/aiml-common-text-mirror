In artificial neural networks, attention is a technique that is meant to
mimic cognitive attention. This effect enhances some parts of the input
data while diminishing other parts --- the motivation being that the
network should devote more focus to the important parts of the data,
even though they may be small portion of an image or sentence. Learning
which part of the data is more important than another depends on the
context, and this is trained by gradient descent. Attention-like
mechanisms were introduced in the 1990s under names like multiplicative
modules, sigma pi units, and hyper-networks. Its flexibility comes from
its role as \"soft weights\" that can change during runtime, in contrast
to standard weights that must remain fixed at runtime. Uses of attention
include memory in fast weight controllers that can learn \"internal
spotlights of attention\" (also known as transformers with \"linearized
self-attention\"), neural Turing machines, reasoning tasks in
differentiable neural computers, language processing in transformers,
and LSTMs, and multi-sensory data processing (sound, images, video, and
text) in perceivers. Approach Correlating the different parts within a
sentence or a picture can help capture its structure. The attention
scheme gives a neural network an opportunity to do that. For example, in
the sentence \"See that girl run.\", when the network processes \"that\"
we want it to know that this word refers to \"girl\". The next diagram
shows how a well trained network can make this happen. In practice,
instead of computing the Context for each word individually as was done
for \"that\", we would do it for all words in the sentence in parallel.
For the example here, this would add a second dimension of size 4 to
each of these vectors: x, q, soft weights, and context. This is much
faster than the sequential operations of recurrent networks used before
Attention. Notation: the commonly written row-wise softmax formula here
assumes that vectors are rows, which contradicts the standard math
notation of column vectors. Strictly speaking, we should take the
transpose of the context vector and use the column-wise softmax,
resulting in the more correct form Context = (XVW)T \* softmax( (KW XT)
\* (xQw)T / sqrt(100) ). A language translation example To build a
machine that translates English to French, an attention unit is grafted
to the basic Encoder-Decoder (diagram below). In the simplest case, the
attention unit consists of dot products of the recurrent encoder states
and does not need training. In practice, the attention unit consists of
3 trained, fully-connected neural network layers called query, key, and
value. Viewed as a matrix, the attention weights show how the network
adjusts its focus according to context. This view of the attention
weights addresses the neural network \"explainability\" problem.
Networks that perform verbatim translation without regard to word order
would show the highest scores along the (dominant) diagonal of the
matrix. The off-diagonal dominance shows that the attention mechanism is
more nuanced. On the first pass through the decoder, 94% of the
attention weight is on the first English word \"I\", so the network
offers the word \"je\". On the second pass of the decoder, 88% of the
attention weight is on the third English word \"you\", so it offers
\"t\'\". On the last pass, 95% of the attention weight is on the second
English word \"love\", so it offers \"aime\". Variants Many variants of
attention implement soft weights, such as \"internal spotlights of
attention\" generated by fast weight programmers or fast weight
controllers (1992) (also \"linearized self-attention\"). A slow neural
network learns by gradient descent to program the fast weights of
another neural network through outer products of self-generated
activation patterns called \"FROM\" and \"TO\" which in transformer
terminology are called \"key\" and \"value.\" This fast weight
\"attention mapping\" is applied to queries. Bahdanau Attention, also
referred to as additive attention, Luong Attention which is known as
multiplicative attention, built on top of additive attention, highly
parallelizable self-attention introduced in 2016 as decomposable
attention and successfully used in transformers a year later.For
convolutional neural networks, attention mechanisms can be distinguished
by the dimension on which they operate, namely: spatial attention,
channel attention, or combinations.These variants recombine the
encoder-side inputs to redistribute those effects to each target output.
Often, a correlation-style matrix of dot products provides the
re-weighting coefficients. Self-attention Self-attention has gained
currency given its use in transformer-based large language models. It
captures dependencies between the different parts of a (single)
sequence. The term \"self\" signals that attention is contained within a
single sequence, without engaging other sequences. The model can capture
dependencies across the entire sequence, without requiring fixed or
sliding windows that consider only part of the sequence at a time.
Self-attention is computationally efficient compared to recurrent neural
networks, which process sequences sequentially. See also Transformer
(machine learning model) ยง Scaled dot-product attention Perceiver ยง
Components for query-key-value (QKV) attention References External links
Dan Jurafsky and James H. Martin (2022) Speech and Language Processing
(3rd ed. draft, January 2022), ch. 10.4 Attention and ch. 9.7
Self-Attention Networks: Transformers Alex Graves (4 May 2020),
Attention and Memory in Deep Learning (video lecture), DeepMind / UCL,
via YouTube Rasa Algorithm Whiteboard - Attention via YouTube
