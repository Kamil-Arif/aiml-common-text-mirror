Existential risk from artificial general intelligence is the hypothesis
that substantial progress in artificial general intelligence (AGI) could
result in human extinction or some other unrecoverable global
catastrophe.The existential risk (\"x-risk\") school argues as follows:
The human species currently dominates other species because the human
brain has some distinctive capabilities that other animals lack. If AI
surpasses humanity in general intelligence and becomes
\"superintelligent\", then it could become difficult or impossible for
humans to control. Just as the fate of the mountain gorilla depends on
human goodwill, so might the fate of humanity depend on the actions of a
future machine superintelligence.The probability of this type of
scenario is widely debated, and hinges in part on differing scenarios
for future progress in computer science. Concerns about
superintelligence have been voiced by leading computer scientists and
tech CEOs such as Geoffrey Hinton, Alan Turing, Elon Musk, and OpenAI
CEO Sam Altman. In 2022, a survey of AI researchers found that some
researchers believe that there is a 10 percent or greater chance that
our inability to control AI will cause an existential catastrophe (more
than half the respondents of the survey, with a 17% response rate).Two
sources of concern are the problems of AI control and alignment: that
controlling a superintelligent machine, or instilling it with
human-compatible values, may be a harder problem than na√Øvely supposed.
Many researchers believe that a superintelligence would resist attempts
to shut it off or change its goals (as such an incident would prevent it
from accomplishing its present goals) and that it will be extremely
difficult to align superintelligence with the full breadth of important
human values and constraints. In contrast, skeptics such as computer
scientist Yann LeCun argue that superintelligent machines will have no
desire for self-preservation.A third source of concern is that a sudden
\"intelligence explosion\" might take an unprepared human race by
surprise. To illustrate, if the first generation of a computer program
that is able to broadly match the effectiveness of an AI researcher can
rewrite its algorithms and double its speed or capabilities in six
months, then the second-generation program is expected to take three
calendar months to perform a similar chunk of work. In this scenario the
time for each generation continues to shrink, and the system undergoes
an unprecedentedly large number of generations of improvement in a short
time interval, jumping from subhuman performance in many areas to
superhuman performance in virtually all domains of interest.
Empirically, examples like AlphaZero in the domain of Go show that AI
systems can sometimes progress from narrow human-level ability to narrow
superhuman ability extremely rapidly. History One of the earliest
authors to express serious concern that highly advanced machines might
pose existential risks to humanity was the novelist Samuel Butler, who
wrote the following in his 1863 essay Darwin among the Machines: The
upshot is simply a question of time, but that the time will come when
the machines will hold the real supremacy over the world and its
inhabitants is what no person of a truly philosophic mind can for a
moment question. In 1951, computer scientist Alan Turing wrote an
article titled Intelligent Machinery, A Heretical Theory, in which he
proposed that artificial general intelligences would likely \"take
control\" of the world as they became more intelligent than human
beings: Let us now assume, for the sake of argument, that
\[intelligent\] machines are a genuine possibility, and look at the
consequences of constructing them\... There would be no question of the
machines dying, and they would be able to converse with each other to
sharpen their wits. At some stage therefore we should have to expect the
machines to take control, in the way that is mentioned in Samuel
Butler\'s Erewhon. In 1965, I. J. Good originated the concept now known
as an \"intelligence explosion\"; he also stated that the risks were
underappreciated: Let an ultraintelligent machine be defined as a
machine that can far surpass all the intellectual activities of any man
however clever. Since the design of machines is one of these
intellectual activities, an ultraintelligent machine could design even
better machines; there would then unquestionably be an \'intelligence
explosion\', and the intelligence of man would be left far behind. Thus
the first ultraintelligent machine is the last invention that man need
ever make, provided that the machine is docile enough to tell us how to
keep it under control. It is curious that this point is made so seldom
outside of science fiction. It is sometimes worthwhile to take science
fiction seriously. Occasional statements from scholars such as Marvin
Minsky and I. J. Good himself expressed philosophical concerns that a
superintelligence could seize control, but contained no call to action.
In 2000, computer scientist and Sun co-founder Bill Joy penned an
influential essay, \"Why The Future Doesn\'t Need Us\", identifying
superintelligent robots as a high-tech danger to human survival,
alongside nanotechnology and engineered bioplagues.In 2009, experts
attended a private conference hosted by the Association for the
Advancement of Artificial Intelligence (AAAI) to discuss whether
computers and robots might be able to acquire any sort of autonomy, and
how much these abilities might pose a threat or hazard. They noted that
some robots have acquired various forms of semi-autonomy, including
being able to find power sources on their own and being able to
independently choose targets to attack with weapons. They also noted
that some computer viruses can evade elimination and have achieved
\"cockroach intelligence\". They concluded that self-awareness as
depicted in science fiction is probably unlikely, but that there were
other potential hazards and pitfalls. The New York Times summarized the
conference\'s view as \"we are a long way from Hal, the computer that
took over the spaceship in 2001: A Space Odyssey\".Nick Bostrom
published Superintelligence in 2014, which presented his arguments that
superintelligence poses an existential threat. By 2015, public figures
such as physicists Stephen Hawking and Nobel laureate Frank Wilczek,
computer scientists Stuart J. Russell and Roman Yampolskiy, and
entrepreneurs Elon Musk and Bill Gates were expressing concern about the
risks of superintelligence. In April 2016, Nature warned: \"Machines and
robots that outperform humans across the board could self-improve beyond
our control---and their interests might not align with ours.\"In 2020,
Brian Christian published The Alignment Problem, which detailed the
history of progress on AI alignment up to that time. General argument
The three difficulties Artificial Intelligence: A Modern Approach, the
standard undergraduate AI textbook, assesses that superintelligence
\"might mean the end of the human race\". It states: \"Almost any
technology has the potential to cause harm in the wrong hands, but with
\[superintelligence\], we have the new problem that the wrong hands
might belong to the technology itself.\" Even if the system designers
have good intentions, two difficulties are common to both AI and non-AI
computer systems: The system\'s implementation may contain
initially-unnoticed but subsequently catastrophic bugs. An analogy is
space probes: despite the knowledge that bugs in expensive space probes
are hard to fix after launch, engineers have historically not been able
to prevent catastrophic bugs from occurring. No matter how much time is
put into pre-deployment design, a system\'s specifications often result
in unintended behavior the first time it encounters a new scenario. For
example, Microsoft\'s Tay behaved inoffensively during pre-deployment
testing, but was too easily baited into offensive behavior when it
interacted with real users.AI systems uniquely add a third problem: that
even given \"correct\" requirements, bug-free implementation, and
initial good behavior, an AI system\'s dynamic learning capabilities may
cause it to evolve into a system with unintended behavior, even without
unanticipated external scenarios. An AI may partly botch an attempt to
design a new generation of itself and accidentally create a successor AI
that is more powerful than itself, but that no longer maintains the
human-compatible moral values preprogrammed into the original AI. For a
self-improving AI to be completely safe, it would not only need to be
bug-free, but it would need to be able to design successor systems that
are also bug-free.All three of these difficulties become catastrophes
rather than nuisances in any scenario where the superintelligence
labeled as \"malfunctioning\" correctly predicts that humans will
attempt to shut it off, and successfully deploys its superintelligence
to outwit such attempts: a scenario that has been given the name
\"treacherous turn\".Citing major advances in the field of AI and the
potential for AI to have enormous long-term benefits or costs, the 2015
Open Letter on Artificial Intelligence stated: The progress in AI
research makes it timely to focus research not only on making AI more
capable, but also on maximizing the societal benefit of AI. Such
considerations motivated the AAAI 2008-09 Presidential Panel on
Long-Term AI Futures and other projects on AI impacts, and constitute a
significant expansion of the field of AI itself, which up to now has
focused largely on techniques that are neutral with respect to purpose.
We recommend expanded research aimed at ensuring that increasingly
capable AI systems are robust and beneficial: our AI systems must do
what we want them to do. Signatories included AAAI president Thomas
Dietterich, Eric Horvitz, Bart Selman, Francesca Rossi, Yann LeCun, and
the founders of Vicarious and Google DeepMind. Bostrom\'s argument A
superintelligent machine would be as alien to humans as human thought
processes are to cockroaches, Bostrom argues. Such a machine may not
have humanity\'s best interests at heart; it is not obvious that it
would even care about human welfare at all. If superintelligent AI is
possible, and if it is possible for a superintelligence\'s goals to
conflict with basic human values, then AI poses a risk of human
extinction. A \"superintelligence\" (a system that exceeds the
capabilities of humans in all domains of interest) can outmaneuver
humans any time its goals conflict with human goals; therefore, unless
the superintelligence decides to allow humanity to coexist, the first
superintelligence to be created will inexorably result in human
extinction.Stephen Hawking argues that there is no physical law
precluding particles from being organised in ways that perform even more
advanced computations than the arrangements of particles in human
brains; therefore, superintelligence is physically possible. In addition
to potential algorithmic improvements over human brains, a digital brain
can be many orders of magnitude larger and faster than a human brain,
which was constrained in size by evolution to be small enough to fit
through a birth canal. Hawking warns that the emergence of
superintelligence may take the human race by surprise, especially if an
intelligence explosion occurs.According to Bostrom\'s \"x-risk school of
thought\", one hypothetical intelligence explosion scenario runs as
follows: An AI gains an expert-level capability at certain key software
engineering tasks. (It may initially lack human or superhuman
capabilities in other domains not directly relevant to engineering.) Due
to its capability to recursively improve its own algorithms, the AI
quickly becomes superhuman; just as human experts can eventually
creatively overcome \"diminishing returns\" by deploying various human
capabilities for innovation, so too can the expert-level AI use either
human-style capabilities or its own AI-specific capabilities to power
through new creative breakthroughs. The AI then possesses intelligence
far surpassing that of the brightest and most gifted human minds in
practically every relevant field, including scientific creativity,
strategic planning, and social skills.The x-risk school believes that
almost any AI, no matter its programmed goal, would rationally prefer to
be in a position where nobody else can switch it off without its
consent: A superintelligence will gain self-preservation as a subgoal as
soon as it realizes that it cannot achieve its goal if it is shut off.
Unfortunately, any compassion for defeated humans whose cooperation is
no longer necessary would be absent in the AI, unless somehow
preprogrammed in. A superintelligent AI will not have a natural drive to
aid humans, for the same reason that humans have no natural desire to
aid AI systems that are of no further use to them. (Another analogy is
that humans seem to have little natural desire to go out of their way to
aid viruses, termites, or even gorillas.) Once in charge, the
superintelligence will have little incentive to allow humans to run
around free and consume resources that the superintelligence could
instead use for building itself additional protective systems \"just to
be on the safe side\" or for building additional computers to help it
calculate how to best accomplish its goals.Thus, the x-risk school
concludes, it is likely that someday an intelligence explosion will
catch humanity unprepared, and may result in human extinction or a
comparable fate. Possible scenarios Some scholars have proposed
hypothetical scenarios to illustrate some of their concerns. In
Superintelligence, Nick Bostrom expresses concern that even if the
timeline for superintelligence turns out to be predictable, researchers
might not take sufficient safety precautions, in part because \"it could
be the case that when dumb, smarter is safe; yet when smart, smarter is
more dangerous\". Bostrom suggests a scenario where, over decades, AI
becomes more powerful. Widespread deployment is initially marred by
occasional accidents---a driverless bus swerves into the oncoming lane,
or a military drone fires into an innocent crowd. Many activists call
for tighter oversight and regulation, and some even predict impending
catastrophe. But as development continues, the activists are proven
wrong. As automotive AI becomes smarter, it suffers fewer accidents; as
military robots achieve more precise targeting, they cause less
collateral damage. Based on the data, scholars mistakenly infer a broad
lesson: the smarter the AI, the safer it is. \"And so we boldly
go---into the whirling knives\", as the superintelligent AI takes a
\"treacherous turn\" and exploits a decisive strategic advantage.In Max
Tegmark\'s 2017 book Life 3.0, a corporation\'s \"Omega team\" creates
an extremely powerful AI able to moderately improve its own source code
in a number of areas. After a certain point the team chooses to publicly
downplay the AI\'s ability, in order to avoid regulation or confiscation
of the project. For safety, the team keeps the AI in a box where it is
mostly unable to communicate with the outside world, and uses it to make
money, by diverse means such as Amazon Mechanical Turk tasks, production
of animated films and TV shows, and development of biotech drugs, with
profits invested back into further improving AI. The team next tasks the
AI with astroturfing an army of pseudonymous citizen journalists and
commentators, in order to gain political influence to use \"for the
greater good\" to prevent wars. The team faces risks that the AI could
try to escape by inserting \"backdoors\" in the systems it designs, by
hidden messages in its produced content, or by using its growing
understanding of human behavior to persuade someone into letting it
free. The team also faces risks that its decision to box the project
will delay the project long enough for another project to overtake
it.Physicist Michio Kaku, an AI risk skeptic, posits a deterministically
positive outcome. In Physics of the Future he asserts that \"It will
take many decades for robots to ascend\" up a scale of consciousness,
and that in the meantime corporations such as Hanson Robotics will
likely succeed in creating robots that are \"capable of love and earning
a place in the extended human family\". AI takeover Anthropomorphic
arguments Anthropomorphic arguments assume that, as machines become more
intelligent, they will begin to display many human traits, such as
morality or a thirst for power. Although anthropomorphic scenarios are
common in fiction, they are rejected by most scholars writing about the
existential risk of artificial intelligence. Instead, AI are modeled as
intelligent agents.The academic debate is between one side which worries
whether AI might destroy humanity and another side which believes that
AI would not destroy humanity at all. Both sides have claimed that the
others\' predictions about an AI\'s behavior are illogical
anthropomorphism. The skeptics accuse proponents of anthropomorphism for
believing an AGI would naturally desire power; proponents accuse some
skeptics of anthropomorphism for believing an AGI would naturally value
human ethical norms.Evolutionary psychologist Steven Pinker, a skeptic,
argues that \"AI dystopias project a parochial alpha-male psychology
onto the concept of intelligence. They assume that superhumanly
intelligent robots would develop goals like deposing their masters or
taking over the world\"; perhaps instead \"artificial intelligence will
naturally develop along female lines: fully capable of solving problems,
but with no desire to annihilate innocents or dominate the
civilization.\" Facebook\'s director of AI research, Yann LeCun states
that \"Humans have all kinds of drives that make them do bad things to
each other, like the self-preservation instinct\... Those drives are
programmed into our brain but there is absolutely no reason to build
robots that have the same kind of drives\".Despite other differences,
the x-risk school agrees with Pinker that an advanced AI would not
destroy humanity out of human emotions such as \"revenge\" or \"anger\",
that questions of consciousness are not relevant to assess the risks,
and that computer systems do not generally have a computational
equivalent of testosterone. They think that power-seeking or
self-preservation behaviors emerge in the AI as a way to achieve its
true goals, according to the concept of instrumental convergence.
Definition of \"intelligence\" According to Bostrom, outside of the
artificial intelligence field, \"intelligence\" is often used to in a
manner that connotes moral wisdom or acceptance of agreeable forms of
moral reasoning. At an extreme, if morality is part of the definition of
intelligence, then by definition a superintelligent machine would behave
morally. However, most \"artificial intelligence\" research instead
focuses on creating algorithms that \"optimize\", in an empirical way,
the achievement of whichever goal the given researchers have
specified.To avoid anthropomorphism or the baggage of the word
\"intelligence\", an advanced artificial intelligence can be thought of
as an impersonal \"optimizing process\" that strictly takes whatever
actions it judges to be most likely to accomplish its (possibly
complicated and implicit) goals. Another way of conceptualizing an
advanced artificial intelligence is to imagine a time machine that sends
backward in time information about which choice always leads to the
maximization of its goal function; this choice is then outputted,
regardless of any extraneous ethical concerns. Sources of risk AI
alignment problem Difficulty of specifying goals In the \"intelligent
agent\" model, an AI can loosely be viewed as a machine that chooses
whatever action appears to best achieve the AI\'s set of goals, or
\"utility function\". A utility function associates to each possible
situation a score that indicates its desirability to the agent.
Researchers know how to write utility functions that mean \"minimize the
average network latency in this specific telecommunications model\" or
\"maximize the number of reward clicks\"; however, they do not know how
to write a utility function for \"maximize human flourishing\", nor is
it currently clear whether such a function meaningfully and
unambiguously exists. Furthermore, a utility function that expresses
some values but not others will tend to trample over the values not
reflected by the utility function. AI researcher Stuart Russell writes:
The primary concern is not spooky emergent consciousness but simply the
ability to make high-quality decisions. Here, quality refers to the
expected outcome utility of actions taken, where the utility function
is, presumably, specified by the human designer. Now we have a problem:
The utility function may not be perfectly aligned with the values of the
human race, which are (at best) very difficult to pin down. Any
sufficiently capable intelligent system will prefer to ensure its own
continued existence and to acquire physical and computational resources
--- not for their own sake, but to succeed in its assigned task.A system
that is optimizing a function of n variables, where the objective
depends on a subset of size k100 years) exceeding the median (a few
decades).In his 2020 book, The Precipice: Existential Risk and the
Future of Humanity, Toby Ord, a Senior Research Fellow at Oxford
University\'s Future of Humanity Institute, estimates the total
existential risk from unaligned AI over the next hundred years to be
about one in ten.Skeptics who believe it is impossible for AGI to arrive
anytime soon tend to argue that expressing concern about existential
risk from AI is unhelpful because it could distract people from more
immediate concerns about the impact of AGI, because of fears it could
lead to government regulation or make it more difficult to secure
funding for AI research, or because it could give AI research a bad
reputation. Some researchers, such as Oren Etzioni, aggressively seek to
quell concern over existential risk from AI, saying \"\[Elon Musk\] has
impugned us in very strong language saying we are unleashing the demon,
and so we\'re answering.\"In 2014, Slate\'s Adam Elkus argued \"our
\'smartest\' AI is about as intelligent as a toddler---and only when it
comes to instrumental tasks like information recall. Most roboticists
are still trying to get a robot hand to pick up a ball or run around
without falling over.\" Elkus goes on to argue that Musk\'s \"summoning
the demon\" analogy may be harmful because it could result in \"harsh
cuts\" to AI research budgets.The Information Technology and Innovation
Foundation (ITIF), a Washington, D.C. think-tank, awarded its 2015
Annual Luddite Award to \"alarmists touting an artificial intelligence
apocalypse\"; its president, Robert D. Atkinson, complained that Musk,
Hawking and AI experts say AI is the largest existential threat to
humanity. Atkinson stated \"That\'s not a very winning message if you
want to get AI funding out of Congress to the National Science
Foundation.\" Nature sharply disagreed with the ITIF in an April 2016
editorial, siding instead with Musk, Hawking, and Russell, and
concluding: \"It is crucial that progress in technology is matched by
solid, well-funded research to anticipate the scenarios it could bring
about\... If that is a Luddite perspective, then so be it.\" In a 2015
The Washington Post editorial, researcher Murray Shanahan stated that
human-level AI is unlikely to arrive \"anytime soon\", but that
nevertheless \"the time to start thinking through the consequences is
now.\" Perspectives The thesis that AI could pose an existential risk
provokes a wide range of reactions within the scientific community, as
well as in the public at large. Many of the opposing viewpoints,
however, share common ground. The Asilomar AI Principles, which contain
only those principles agreed to by 90% of the attendees of the Future of
Life Institute\'s Beneficial AI 2017 conference, agree in principle that
\"There being no consensus, we should avoid strong assumptions regarding
upper limits on future AI capabilities\" and \"Advanced AI could
represent a profound change in the history of life on Earth, and should
be planned for and managed with commensurate care and resources.\" AI
safety advocates such as Bostrom and Tegmark have criticized the
mainstream media\'s use of \"those inane Terminator pictures\" to
illustrate AI safety concerns: \"It can\'t be much fun to have
aspersions cast on one\'s academic discipline, one\'s professional
community, one\'s life work \... I call on all sides to practice
patience and restraint, and to engage in direct dialogue and
collaboration as much as possible.\"Conversely, many skeptics agree that
ongoing research into the implications of artificial general
intelligence is valuable. Skeptic Martin Ford states that \"I think it
seems wise to apply something like Dick Cheney\'s famous \'1 Percent
Doctrine\' to the specter of advanced artificial intelligence: the odds
of its occurrence, at least in the foreseeable future, may be very
low---but the implications are so dramatic that it should be taken
seriously\". Similarly, an otherwise skeptical Economist stated in 2014
that \"the implications of introducing a second intelligent species onto
Earth are far-reaching enough to deserve hard thinking, even if the
prospect seems remote\".A 2014 survey showed the opinion of experts
within the field of artificial intelligence is mixed, with sizable
fractions both concerned and unconcerned by risk from eventual
superhumanly-capable AI. A 2017 email survey of researchers with
publications at the 2015 NIPS and ICML machine learning conferences
asked them to evaluate Stuart J. Russell\'s concerns about AI risk. Of
the respondents, 5% said it was \"among the most important problems in
the field\", 34% said it was \"an important problem\", and 31% said it
was \"moderately important\", whilst 19% said it was \"not important\"
and 11% said it was \"not a real problem\" at all. Preliminary results
of a 2022 expert survey with a 17% response rate appear to show median
responses around five or ten percent when asked to estimate the
probability of human extinction from artificial intelligence.
Endorsement The thesis that AI poses an existential risk, and that this
risk needs much more attention than it currently gets, has been endorsed
by many computer scientists and public figures including Alan Turing,,
the most-cited computer scientist Geoffrey Hinton, Elon Musk, OpenAI CEO
Sam Altman, Bill Gates, and Stephen Hawking. Endorsers of the thesis
sometimes express bafflement at skeptics: Gates states that he does not
\"understand why some people are not concerned\", and Hawking criticized
widespread indifference in his 2014 editorial: So, facing possible
futures of incalculable benefits and risks, the experts are surely doing
everything possible to ensure the best outcome, right? Wrong. If a
superior alien civilisation sent us a message saying, \'We\'ll arrive in
a few decades,\' would we just reply, \'OK, call us when you get
here---we\'ll leave the lights on?\' Probably not---but this is more or
less what is happening with AI. Concern over risk from artificial
intelligence has led to some high-profile donations and investments. In
2015, Peter Thiel, Amazon Web Services, and Musk and others jointly
committed \$1 billion to OpenAI, consisting of a for-profit corporation
and the nonprofit parent company which states that it is aimed at
championing responsible AI development. Facebook co-founder Dustin
Moskovitz has funded and seeded multiple labs working on AI Alignment,
notably \$5.5 million in 2016 to launch the Centre for Human-Compatible
AI led by Professor Stuart Russell. In January 2015, Elon Musk donated
\$10 million to the Future of Life Institute to fund research on
understanding AI decision making. The goal of the institute is to \"grow
wisdom with which we manage\" the growing power of technology. Musk also
funds companies developing artificial intelligence such as DeepMind and
Vicarious to \"just keep an eye on what\'s going on with artificial
intelligence, saying \"I think there is potentially a dangerous outcome
there.\" Skepticism The thesis that AI can pose existential risk has
many detractors. Skeptics sometimes charge that the thesis is
crypto-religious, with an irrational belief in the possibility of
superintelligence replacing an irrational belief in an omnipotent God.
Jaron Lanier argued in 2014 that the whole concept that then-current
machines were in any way intelligent was \"an illusion\" and a
\"stupendous con\" by the wealthy.Some criticism argues that AGI is
unlikely in the short term. AI researcher Rodney Brooks wrote in 2014,
\"I think it is a mistake to be worrying about us developing malevolent
AI anytime in the next few hundred years. I think the worry stems from a
fundamental error in not distinguishing the difference between the very
real recent advances in a particular aspect of AI and the enormity and
complexity of building sentient volitional intelligence.\" Baidu Vice
President Andrew Ng stated in 2015 that AI existential risk is \"like
worrying about overpopulation on Mars when we have not even set foot on
the planet yet.\" Computer scientist Gordon Bell argued in 2008 that the
human race will destroy itself before it reaches the technological
singularity. Gordon Moore, the original proponent of Moore\'s Law,
declares that \"I am a skeptic. I don\'t believe \[a technological
singularity\] is likely to happen, at least for a long time. And I
don\'t know why I feel that way.\"For the danger of uncontrolled
advanced AI to be realized, the hypothetical AI may have to overpower or
out-think any human, which some experts argue is a possibility far
enough in the future to not be worth researching. The economist Robin
Hanson considers that, to launch an intelligence explosion, the AI would
have to become vastly better at software innovation than all the rest of
the world combined, which looks implausible to him.Another line of
criticism posits that intelligence is only one component of a much
broader ability to achieve goals. Magnus Vinding argues that "advanced
goal-achieving abilities, including abilities to build new tools,
require many tools, and our cognitive abilities are just a subset of
these tools. Advanced hardware, materials, and energy must all be
acquired if any advanced goal is to be achieved." Vinding further argues
that "what we consistently observe \[in history\] is that, as
goal-achieving systems have grown more competent, they have grown ever
more dependent on an ever larger, ever more distributed system." Vinding
writes that there is no reason to expect the trend to reverse,
especially for machines, which "depend on materials, tools, and know-how
distributed widely across the globe for their construction and
maintenance". Such arguments lead Vinding to think that there is no
"concentrated center of capability" and thus no "grand control problem".
The futurist Max More considers that even if a superintelligence did
emerge, it would be limited by the speed of the rest of the world and
thus prevented from taking over the economy in an uncontrollable
manner:Unless full-blown nanotechnology and robotics appear before the
superintelligence, \[\...\] The need for collaboration, for
organization, and for putting ideas into physical changes will ensure
that all the old rules are not thrown out overnight or even within
years. Superintelligence may be difficult to achieve. It may come in
small steps, rather than in one history-shattering burst. Even a greatly
advanced SI won\'t make a dramatic difference in the world when compared
with billions of augmented humans increasingly integrated with
technology \[\...\]The chaotic nature or time complexity of some systems
could also fundamentally limit the ability of a superintelligence to
predict some aspects of the future, increasing its uncertainty.Some AI
and AGI researchers may be reluctant to discuss risks, worrying that
policymakers do not have sophisticated knowledge of the field and are
prone to be convinced by \"alarmist\" messages, or worrying that such
messages will lead to cuts in AI funding. Slate notes that some
researchers are dependent on grants from government agencies such as
DARPA.Several skeptics argue that the potential near-term benefits of AI
outweigh the risks. Facebook CEO Mark Zuckerberg believes AI will
\"unlock a huge amount of positive things\", such as curing disease and
increasing the safety of autonomous cars. Intermediate views
Intermediate views generally take the position that the control problem
of artificial general intelligence may exist, but that it will be solved
via progress in artificial intelligence, for example by creating a moral
learning environment for the AI, taking care to spot clumsy malevolent
behavior (the \"sordid stumble\") and then directly intervening in the
code before the AI refines its behavior, or even peer pressure from
friendly AIs. In a 2015 panel discussion in The Wall Street Journal
devoted to AI risks, IBM\'s vice-president of Cognitive Computing,
Guruduth S. Banavar, brushed off discussion of AGI with the phrase, \"it
is anybody\'s speculation.\" Geoffrey Hinton, the \"godfather of deep
learning\", noted that \"there is not a good track record of less
intelligent things controlling things of greater intelligence\", but
stated that he continues his research because \"the prospect of
discovery is too sweet\". Asked about the possibility of an AI trying to
eliminate the human race, Hinton has stated such a scenario was \"not
inconceivable\", but the bigger issue with an \"intelligence explosion\"
would be the resultant concentration of power. In 2004, law professor
Richard Posner wrote that dedicated efforts for addressing AI can wait,
but that we should gather more information about the problem in the
meanwhile. Popular reaction In a 2014 article in The Atlantic, James
Hamblin noted that most people do not care about artificial general
intelligence, and characterized his own gut reaction to the topic as:
\"Get out of here. I have a hundred thousand things I am concerned about
at this exact moment. Do I seriously need to add to that a technological
singularity?\" During a 2016 Wired interview of President Barack Obama
and MIT Media Lab\'s Joi Ito, Ito stated: There are a few people who
believe that there is a fairly high-percentage chance that a generalized
AI will happen in the next 10 years. But the way I look at it is that in
order for that to happen, we\'re going to need a dozen or two different
breakthroughs. So you can monitor when you think these breakthroughs
will happen. Obama added: And you just have to have somebody close to
the power cord. \[Laughs.\] Right when you see it about to happen, you
gotta yank that electricity out of the wall, man. Hillary Clinton stated
in What Happened: Technologists\... have warned that artificial
intelligence could one day pose an existential security threat. Musk has
called it \"the greatest risk we face as a civilization\". Think about
it: Have you ever seen a movie where the machines start thinking for
themselves that ends well? Every time I went out to Silicon Valley
during the campaign, I came home more alarmed about this. My staff lived
in fear that I\'d start talking about \"the rise of the robots\" in some
Iowa town hall. Maybe I should have. In any case, policy makers need to
keep up with technology as it races ahead, instead of always playing
catch-up. In a 2016 YouGov poll of the public for the British Science
Association, about a third of survey respondents said AI will pose a
threat to the long-term survival of humanity. Slate\'s Jacob Brogan
stated that \"most of the (readers filling out our online survey) were
unconvinced that A.I. itself presents a direct threat.\"In 2018, a
SurveyMonkey poll of the American public by USA Today found 68% thought
the real current threat remains \"human intelligence\"; however, the
poll also found that 43% said superintelligent AI, if it were to happen,
would result in \"more harm than good\", and 38% said it would do
\"equal amounts of harm and good\".One techno-utopian viewpoint
expressed in some popular fiction is that AGI may tend towards
peace-building. Mitigation Many scholars concerned about the AGI
existential risk believe that the best approach is to conduct
substantial research into solving the difficult \"control problem\":
what types of safeguards, algorithms, or architectures can programmers
implement to maximize the probability that their recursively-improving
AI would continue to behave in a friendly manner after it reaches
superintelligence? Social measures may mitigate the AGI existential
risk; for instance, one recommendation is for a UN-sponsored
\"Benevolent AGI Treaty\" that would ensure only altruistic AGIs be
created. Similarly, an arms control approach has been suggested, as has
a global peace treaty grounded in the international relations theory of
conforming instrumentalism, with an ASI potentially being a
signatory.Researchers at Google have proposed research into general \"AI
safety\" issues to simultaneously mitigate both short-term risks from
narrow AI and long-term risks from AGI. A 2020 estimate places global
spending on AI existential risk somewhere between \$10 and \$50 million,
compared with global spending on AI around perhaps \$40 billion. Bostrom
suggests a general principle of \"differential technological
development\": that funders should speed up the development of
protective technologies relative to the development of dangerous ones.
Some funders, such as Elon Musk, propose that radical human cognitive
enhancement could be such a technology, for example direct neural
linking between human and machine; however, others argue that
enhancement technologies may themselves pose an existential risk.
Researchers, if they are not caught off-guard, could closely monitor or
attempt to box in an initial AI at a risk of becoming too powerful, as
an attempt at a stop-gap measure. A dominant superintelligent AI, if it
were aligned with human interests, might itself take action to mitigate
the risk of takeover by rival AI, although the creation of the dominant
AI could itself pose an existential risk.Institutions such as the
Machine Intelligence Research Institute, the Future of Humanity
Institute, the Future of Life Institute, the Centre for the Study of
Existential Risk, and the Center for Human-Compatible AI are involved in
mitigating existential risk from advanced artificial intelligence, for
example by research into friendly artificial intelligence. Views on
banning and regulation Banning Most scholars believe that even if AGI
poses an existential risk, attempting to ban research into artificial
intelligence would still be unwise, and probably futile. Skeptics argue
that regulation of AI would be completely valueless, as no existential
risk exists. However, scholars who believe existential risk proposed
that it is difficult to depend on people from the AI industry to
regulate or constraint AI research because it directly contradict their
personal interests. The scholars also agree with the skeptics that
banning research would be unwise, as research could be moved to
countries with looser regulations or conducted covertly. The latter
issue is particularly relevant, as artificial intelligence research can
be done on a small scale without substantial infrastructure or
resources. Two additional hypothetical difficulties with bans (or other
regulation) are that technology entrepreneurs statistically tend towards
general skepticism about government regulation, and that businesses
could have a strong incentive to (and might well succeed at) fighting
regulation and politicizing the underlying debate. Regulation In March
2023, the Elon Musk-funded Future of Life Institute (FLI) drafted a
letter calling on major AI developers to agree on a verifiable six-month
pause of any systems \"more powerful than GPT-4\" and to use that time
to institute a framework for ensuring safety; or, failing that, for
governments to step in with a moratorium. The letter referred to the
possibility of \"a profound change in the history of life on Earth\" as
well as potential risks of AI-generated propaganda, loss of jobs, human
obsolescence, and society-wide loss of control. Besides Musk, prominent
signatories included Steve Wozniak, Evan Sharp, Chris Larsen, and Gary
Marcus; AI lab CEOs Connor Leahy and Emad Mostaque; politician Andrew
Yang; and deep-learning pioneer Yoshua Bengio. Marcus stated \"the
letter isn\'t perfect, but the spirit is right.\" Mostaque stated \"I
don\'t think a six month pause is the best idea or agree with everything
but there are some interesting things in that letter.\" In contrast,
Bengio explicitly endorsed the six-month pause in a press conference.
Musk stated that \"Leading AGI developers will not heed this warning,
but at least it was said.\" Some signatories, such as Marcus, signed out
of concern about mundane risks such as AI-generated propaganda, rather
than out of concern about superintelligent AGI. Margaret Mitchell, whose
work is cited by the letter, criticised it, saying: "By treating a lot
of questionable ideas as a given, the letter asserts a set of priorities
and a narrative on AI that benefits the supporters of FLI. Ignoring
active harms right now is a privilege that some of us don't have."Musk
called for some sort of regulation of AI development as early as 2017.
According to NPR, the Tesla CEO is \"clearly not thrilled\" to be
advocating for government scrutiny that could impact his own industry,
but believes the risks of going completely without oversight are too
high: \"Normally the way regulations are set up is when a bunch of bad
things happen, there\'s a public outcry, and after many years a
regulatory agency is set up to regulate that industry. It takes forever.
That, in the past, has been bad but not something which represented a
fundamental risk to the existence of civilisation.\" Musk states the
first step would be for the government to gain \"insight\" into the
actual status of current research, warning that \"Once there is
awareness, people will be extremely afraid\... \[as\] they should be.\"
In response, politicians expressed skepticism about the wisdom of
regulating a technology that is still in development.Responding both to
Musk and to February 2017 proposals by European Union lawmakers to
regulate AI and robotics, Intel CEO Brian Krzanich argued that
artificial intelligence is in its infancy and that it is too early to
regulate the technology. Instead of trying to regulate the technology
itself, some scholars suggest common norms including requirements for
the testing and transparency of algorithms, possibly in combination with
some form of warranty. Developing well-regulated weapons systems is in
line with the ethos of some countries\' militaries. On October 31, 2019,
the United States Department of Defense\'s (DoD\'s) Defense Innovation
Board published the draft of a report outlining five principles for
weaponized AI and making 12 recommendations for the ethical use of
artificial intelligence by the DoD that seeks to manage the control
problem in all DoD weaponized AI.Regulation of AGI would likely be
influenced by regulation of weaponized or militarized AI, i.e., the AI
arms race, which is an emerging issue. At present, although the United
Nations is making progress towards regulation of AI, its institutional
and legal capability to manage AGI existential risk is much more
limited. Any form of international regulation will likely be influenced
by developments in leading countries\' domestic policy towards
militarized AI, which in the US is under the purview of the National
Security Commission on Artificial Intelligence, and international moves
to regulate an AI arms race. Regulation of research into AGI focuses on
the role of review boards, encouraging research into safe AI, the
possibility of differential technological progress (prioritizing
risk-reducing strategies over risk-taking strategies in AI development),
or conducting international mass surveillance to perform AGI arms
control. Regulation of conscious AGIs focuses on integrating them with
existing human society and can be divided into considerations of their
legal standing and of their moral rights. AI arms control will likely
require the institutionalization of new international norms embodied in
effective technical specifications combined with active monitoring and
informal diplomacy by communities of experts, together with a legal and
political verification process. See also Notes References Bibliography
Clark, Jack (2015a). \"Musk-Backed Group Probes Risks Behind Artificial
Intelligence\". Bloomberg.com. Archived from the original on 30 October
2015. Retrieved 30 October 2015.
