Predictive modelling uses statistics to predict outcomes. Most often the
event one wants to predict is in the future, but predictive modelling
can be applied to any type of unknown event, regardless of when it
occurred. For example, predictive models are often used to detect crimes
and identify suspects, after the crime has taken place.In many cases the
model is chosen on the basis of detection theory to try to guess the
probability of an outcome given a set amount of input data, for example
given an email determining how likely that it is spam. Models can use
one or more classifiers in trying to determine the probability of a set
of data belonging to another set. For example, a model might be used to
determine whether an email is spam or \"ham\" (non-spam). Depending on
definitional boundaries, predictive modelling is synonymous with, or
largely overlapping with, the field of machine learning, as it is more
commonly referred to in academic or research and development contexts.
When deployed commercially, predictive modelling is often referred to as
predictive analytics. Predictive modelling is often contrasted with
causal modelling/analysis. In the former, one may be entirely satisfied
to make use of indicators of, or proxies for, the outcome of interest.
In the latter, one seeks to determine true cause-and-effect
relationships. This distinction has given rise to a burgeoning
literature in the fields of research methods and statistics and to the
common statement that \"correlation does not imply causation\". Models
Nearly any statistical model can be used for prediction purposes.
Broadly speaking, there are two classes of predictive models: parametric
and non-parametric. A third class, semi-parametric models, includes
features of both. Parametric models make \"specific assumptions with
regard to one or more of the population parameters that characterize the
underlying distribution(s)\". Non-parametric models \"typically involve
fewer assumptions of structure and distributional form \[than parametric
models\] but usually contain strong assumptions about independencies\".
Applications Uplift modelling Uplift modelling is a technique for
modelling the change in probability caused by an action. Typically this
is a marketing action such as an offer to buy a product, to use a
product more or to re-sign a contract. For example, in a retention
campaign you wish to predict the change in probability that a customer
will remain a customer if they are contacted. A model of the change in
probability allows the retention campaign to be targeted at those
customers on whom the change in probability will be beneficial. This
allows the retention programme to avoid triggering unnecessary churn or
customer attrition without wasting money contacting people who would act
anyway. Archaeology Predictive modelling in archaeology gets its
foundations from Gordon Willey\'s mid-fifties work in the VirÃº Valley of
Peru. Complete, intensive surveys were performed then covariability
between cultural remains and natural features such as slope and
vegetation were determined. Development of quantitative methods and a
greater availability of applicable data led to growth of the discipline
in the 1960s and by the late 1980s, substantial progress had been made
by major land managers worldwide. Generally, predictive modelling in
archaeology is establishing statistically valid causal or covariable
relationships between natural proxies such as soil types, elevation,
slope, vegetation, proximity to water, geology, geomorphology, etc., and
the presence of archaeological features. Through analysis of these
quantifiable attributes from land that has undergone archaeological
survey, sometimes the \"archaeological sensitivity\" of unsurveyed areas
can be anticipated based on the natural proxies in those areas. Large
land managers in the United States, such as the Bureau of Land
Management (BLM), the Department of Defense (DOD), and numerous highway
and parks agencies, have successfully employed this strategy. By using
predictive modelling in their cultural resource management plans, they
are capable of making more informed decisions when planning for
activities that have the potential to require ground disturbance and
subsequently affect archaeological sites. Customer relationship
management Predictive modelling is used extensively in analytical
customer relationship management and data mining to produce
customer-level models that describe the likelihood that a customer will
take a particular action. The actions are usually sales, marketing and
customer retention related. For example, a large consumer organization
such as a mobile telecommunications operator will have a set of
predictive models for product cross-sell, product deep-sell (or
upselling) and churn. It is also now more common for such an
organization to have a model of savability using an uplift model. This
predicts the likelihood that a customer can be saved at the end of a
contract period (the change in churn probability) as opposed to the
standard churn prediction model. Auto insurance Predictive modelling is
utilised in vehicle insurance to assign risk of incidents to policy
holders from information obtained from policy holders. This is
extensively employed in usage-based insurance solutions where predictive
models utilise telemetry-based data to build a model of predictive risk
for claim likelihood. Black-box auto insurance predictive models utilise
GPS or accelerometer sensor input only. Some models include a wide range
of predictive input beyond basic telemetry including advanced driving
behaviour, independent crash records, road history, and user profiles to
provide improved risk models. Health care In 2009 Parkland Health &
Hospital System began analyzing electronic medical records in order to
use predictive modeling to help identify patients at high risk of
readmission. Initially the hospital focused on patients with congestive
heart failure, but the program has expanded to include patients with
diabetes, acute myocardial infarction, and pneumonia.In 2018, Banerjee
et al. proposed a deep learning model for estimating short-term life
expectancy (\>3 months) of the patients by analyzing free-text clinical
notes in the electronic medical record, while maintaining the temporal
visit sequence. The model was trained on a large dataset (10,293
patients) and validated on a separated dataset (1818 patients). It
achieved an area under the ROC (Receiver Operating Characteristic) curve
of 0.89. To provide explain-ability, they developed an interactive
graphical tool that may improve physician understanding of the basis for
the model\'s predictions. The high accuracy and explain-ability of the
PPES-Met model may enable the model to be used as a decision support
tool to personalize metastatic cancer treatment and provide valuable
assistance to physicians. Predictive modelling has been used to estimate
surgery duration. Algorithmic trading Predictive modeling in trading is
a modeling process wherein the probability of an outcome is predicted
using a set of predictor variables. Predictive models can be built for
different assets like stocks, futures, currencies, commodities etc.
Predictive modeling is still extensively used by trading firms to devise
strategies and trade. It utilizes mathematically advanced software to
evaluate indicators on price, volume, open interest and other historical
data, to discover repeatable patterns. Lead tracking systems Predictive
modelling gives lead generators a head start by forecasting data-driven
outcomes for each potential campaign. This method saves time and exposes
potential blind spots to help client make smarter decisions. Notable
failures of predictive modeling Although not widely discussed by the
mainstream predictive modeling community, predictive modeling is a
methodology that has been widely used in the financial industry in the
past and some of the major failures contributed to the financial crisis
of 2007--2008. These failures exemplify the danger of relying
exclusively on models that are essentially backward looking in nature.
The following examples are by no mean a complete list: Bond rating. S&P,
Moody\'s and Fitch quantify the probability of default of bonds with
discrete variables called rating. The rating can take on discrete values
from AAA down to D. The rating is a predictor of the risk of default
based on a variety of variables associated with the borrower and
historical macroeconomic data. The rating agencies failed with their
ratings on the US\$600 billion mortgage backed Collateralized Debt
Obligation (CDO) market. Almost the entire AAA sector (and the super-AAA
sector, a new rating the rating agencies provided to represent super
safe investment) of the CDO market defaulted or severely downgraded
during 2008, many of which obtained their ratings less than just a year
previously. So far, no statistical models that attempt to predict equity
market prices based on historical data are considered to consistently
make correct predictions over the long term. One particularly memorable
failure is that of Long Term Capital Management, a fund that hired
highly qualified analysts, including a Nobel Memorial Prize in Economic
Sciences winner, to develop a sophisticated statistical model that
predicted the price spreads between different securities. The models
produced impressive profits until a major debacle that caused the then
Federal Reserve chairman Alan Greenspan to step in to broker a rescue
plan by the Wall Street broker dealers in order to prevent a meltdown of
the bond market. Possible fundamental limitations of predictive models
based on data fitting History cannot always accurately predict the
future. Using relations derived from historical data to predict the
future implicitly assumes there are certain lasting conditions or
constants in a complex system. This almost always leads to some
imprecision when the system involves people.Unknown unknowns are an
issue. In all data collection, the collector first defines the set of
variables for which data is collected. However, no matter how extensive
the collector considers his/her selection of the variables, there is
always the possibility of new variables that have not been considered or
even defined, yet are critical to the outcome.Algorithms can be defeated
adversarially. After an algorithm becomes an accepted standard of
measurement, it can be taken advantage of by people who understand the
algorithm and have the incentive to fool or manipulate the outcome. This
is what happened to the CDO rating described above. The CDO dealers
actively fulfilled the rating agencies\' input to reach an AAA or
super-AAA on the CDO they were issuing, by cleverly manipulating
variables that were \"unknown\" to the rating agencies\'
\"sophisticated\" models. See also Calibration (statistics) Prediction
interval Predictive analytics Predictive inference Statistical learning
theory Statistical model References Further reading Clarke, Bertrand S.;
Clarke, Jennifer L. (2018), Predictive Statistics, Cambridge University
Press Iglesias, Pilar; Sandoval, MÃ´nica C.; Pereira, Carlos Alberto de
BraganÃ§a (1993), \"Predictive likelihood in finite populations\",
Brazilian Journal of Probability and Statistics, 7 (1): 65--82, JSTOR
43600831 Kelleher, John D.; Mac Namee, Brian; D\'Arcy, Aoife (2015),
Fundamentals of Machine Learning for Predictive Data Analytics:
Algorithms, worked Examples and Case Studies, MIT Press Kuhn, Max;
Johnson, Kjell (2013), Applied Predictive Modeling, Springer Shmueli, G.
(2010), \"To explain or to predict?\", Statistical Science, 25 (3):
289--310, arXiv:1101.0891, doi:10.1214/10-STS330, S2CID 15900983
